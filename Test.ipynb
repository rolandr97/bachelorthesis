{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports, Variables & Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version:  2.8.0\n",
      "Numpy version:  1.22.2\n",
      "Foolbox version:  3.3.1\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "#Imports\n",
    "import tensorflow as tf\n",
    "from keras.datasets import mnist\n",
    "import foolbox as fb\n",
    "from keras import callbacks\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.datasets import mnist\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from itertools import product\n",
    "from scipy.ndimage.interpolation import rotate, shift\n",
    "import csv\n",
    "\n",
    "\n",
    "\n",
    "#Variables\n",
    "epsilon=0.3\n",
    "batch_size=1024\n",
    "epochs=1000\n",
    "pgd_steps=50\n",
    "batch_count=0\n",
    "batch_count_inv=0\n",
    "print(\"Tensorflow version: \", tf.__version__)\n",
    "print(\"Numpy version: \", np.__version__)\n",
    "print(\"Foolbox version: \", fb.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "np.random.seed(10)\n",
    "\n",
    "\n",
    "#get MNIST data and prepare\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "img_rows = img_cols = 28\n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "#define variables needed for attacks\n",
    "x_attack_to_train = tf.convert_to_tensor(x_train, dtype=tf.float32)\n",
    "x_attack_to_train=x_attack_to_train[:,:,:,np.newaxis]\n",
    "y_attack_to_train=tf.convert_to_tensor(y_train, dtype=tf.int32)\n",
    "\n",
    "x_attack_to_test = tf.convert_to_tensor(x_test, dtype=tf.float32)\n",
    "x_attack_to_test=x_attack_to_test[:,:,:,np.newaxis]\n",
    "y_attack_to_test=tf.convert_to_tensor(y_test, dtype=tf.int32)\n",
    "\n",
    "attack = fb.attacks.projected_gradient_descent.LinfProjectedGradientDescentAttack(steps=pgd_steps)\n",
    "\n",
    "#for generating invariance-based adversarial examples\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Functions\n",
    "def test_model(model):\n",
    "    \n",
    "    assert epsilon==0.3\n",
    "    inv_advs_to_test=np.load(\"data/invariance_examples_tramer/linf/automated_eps03.npy\")[0:100]\n",
    "    inv_labels_to_test=np.load(\"data/invariance_examples_tramer/linf/automated_eps03_labels.npy\")[0:100]\n",
    "    fmodel=fb.models.tensorflow.TensorFlowModel(model, bounds=(0,1))      \n",
    "    \n",
    "\n",
    "    # x_batch,y_batch=(100,x_test,y_test)\n",
    "    x_batch,y_batch=x_test[0:100],y_test[0:100]\n",
    "    x_batch_to_test = tf.convert_to_tensor(x_batch, dtype=tf.float32)\n",
    "    y_batch_to_test=tf.convert_to_tensor(y_batch, dtype=tf.int32)\n",
    "\n",
    "    _,advs_to_test, success=attack(fmodel,x_batch_to_test, y_batch_to_test, epsilons=epsilon)\n",
    "   \n",
    "    \n",
    "    success_rate=tf.keras.backend.get_value(success).mean(axis=-1).round(2)\n",
    "    x=tf.keras.backend.get_value(advs_to_test)\n",
    "    ptb_test=x\n",
    "\n",
    "    #get accuracies and losses\n",
    "    acc =model.evaluate(x_test[0:100],to_categorical(y_test[0:100]), verbose=0)\n",
    "    acc_ptb = model.evaluate(ptb_test,to_categorical(y_batch), verbose=0)\n",
    "    acc_inv = model.evaluate(inv_advs_to_test,to_categorical(inv_labels_to_test), verbose=0)\n",
    "\n",
    "\n",
    "    # get invariance adversarial examples success rate\n",
    "    predictions=model.predict(inv_advs_to_test)\n",
    "    disagreeing=0\n",
    "    for i in range(len(predictions)):\n",
    "        if inv_labels_to_test[i] !=np.argmax(predictions[i]):\n",
    "            disagreeing+=1\n",
    "    # plt.imshow(ptb_test[0], cmap='gray')\n",
    "    # plt.show()  \n",
    "   \n",
    "      \n",
    "    return {\n",
    "    \"clean\":{\"loss\": acc[0], \"accuracy\":acc[1]},\n",
    "    \"ptb\":{\"loss\": acc_ptb[0], \"accuracy\":acc_ptb[1]},\n",
    "    \"inv\":{\"loss\": acc_inv[0], \"accuracy\":acc_inv[1]},\n",
    "    \"inv_success_rate\":disagreeing/100}\n",
    "\n",
    "\n",
    "def create_vanilla_model():\n",
    "      print(\"creating vanilla model...\")\n",
    "      \n",
    "      val_images = x_train[:10000]\n",
    "      partial_images = x_train[10000:]\n",
    "      val_labels = y_train[:10000]\n",
    "      partial_labels = y_train[10000:]\n",
    "\n",
    "      model = Sequential()\n",
    "\n",
    "      model.add(Conv2D(32, (5, 5), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\n",
    "      model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "      model.add(Conv2D(64, (5, 5), activation='relu', kernel_initializer='he_uniform'))\n",
    "      model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "      model.add(Flatten())\n",
    "      model.add(Dense(1024, activation='relu', kernel_initializer='he_uniform'))\n",
    "      model.add(Dense(10, activation='softmax'))\n",
    "     \n",
    "\n",
    "\n",
    "      earlystopping = callbacks.EarlyStopping(monitor =\"val_loss\", \n",
    "                                        mode =\"min\", patience = 1, \n",
    "                                        restore_best_weights = True)\n",
    "\n",
    "      model.compile(optimizer='RMSprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "      print(\"training vanilla model...\")\n",
    "      history=model.fit(partial_images,to_categorical(partial_labels),\n",
    "                  validation_data =(val_images, to_categorical(val_labels)),\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=epochs,\n",
    "                  shuffle=True,\n",
    "                  verbose=2,\n",
    "                  callbacks =[earlystopping]\n",
    "                  )\n",
    "      print(np.shape(x_test))\n",
    "      acc = model.evaluate(x_test[0:100],to_categorical(y_test[0:100]))\n",
    "      print('BEFORE RETRAIN: Accuracy on clean testing data', acc[1])\n",
    "\n",
    "      return model\n",
    "\n",
    "def create_vanilla_model_tramer(filters=64, s1=5, s2=5, s3=3,\n",
    "               d1=0, d2=0, fc=256,\n",
    "               lr=1e-3, decay=1e-3):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters, kernel_size=(s1, s1),\n",
    "                     activation='relu',\n",
    "                     input_shape=(28, 28, 1)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(filters*2, (s2, s2), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(filters*2, (s3, s3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(d1))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(fc, activation='relu'))\n",
    "    model.add(Dropout(d2))\n",
    "    model.add(Dense(10))\n",
    "    \n",
    "   \n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='Adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    final = Sequential()\n",
    "    final.add(model)\n",
    "    final.add(Activation('softmax'))\n",
    "    final.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='Adam',\n",
    "                  metrics=['accuracy'])\n",
    "        \n",
    "    final.fit(x_train, to_categorical(y_train, 10),\n",
    "              batch_size=256,\n",
    "              epochs=20,\n",
    "              shuffle=True,\n",
    "              verbose=2,\n",
    "    )\n",
    "    return final    \n",
    "\n",
    "\n",
    "# https://stackoverflow.com/questions/40994583/how-to-implement-tensorflows-next-batch-for-own-data\n",
    "# Get random batch of data\n",
    "\n",
    "\n",
    "def next_batch(data, labels, data_type):\n",
    "    if data_type==\"mnist\":\n",
    "        global batch_count\n",
    "        start=batch_count*100\n",
    "        end=(batch_count+1)*100\n",
    "        if batch_count<99:\n",
    "            batch_count+=1\n",
    "        else:\n",
    "            batch_count=0\n",
    "        \n",
    "        return data[start:end], labels[start:end]\n",
    "    if data_type==\"inv\":\n",
    "        global batch_count_inv\n",
    "        start=batch_count_inv*100\n",
    "        end=(batch_count_inv+1)*100\n",
    "        if batch_count_inv<4:\n",
    "            batch_count_inv+=1\n",
    "        else:\n",
    "            batch_count_inv=0\n",
    "        \n",
    "        return data[start:end], labels[start:end]\n",
    "\n",
    "\n",
    "\n",
    "# def next_batch(num, data, labels):\n",
    "#     idx = np.arange(0 , len(data))\n",
    "#     np.random.shuffle(idx)\n",
    "#     idx = idx[:num]\n",
    "#     data_shuffle = [data[ i] for i in idx]\n",
    "#     labels_shuffle = [labels[ i] for i in idx]\n",
    "#     return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n",
    "\n",
    "\n",
    "# https://github.com/ftramer/Excessive-Invariance\n",
    "def linf_attack(x, nn_adv, eps):\n",
    "    x_adv = x.copy().astype(np.float32)\n",
    "    nn_adv = nn_adv.astype(np.float32)\n",
    "    \n",
    "    # if possible, change the pixels to the target value\n",
    "    idx = np.where((np.abs(nn_adv - x) <= eps*255.) & (x > 0))\n",
    "    x_adv[idx] = nn_adv[idx]\n",
    "    \n",
    "    # otherwise, go as close as possible\n",
    "    idx = np.where(np.abs(nn_adv - x) > eps*255.)\n",
    "    sign = np.sign(nn_adv - x)\n",
    "    x_adv[idx] += sign[idx] * eps * 255.\n",
    "    \n",
    "    x_adv = np.clip(x_adv, x.astype(np.float32) - eps*255, x.astype(np.float32) + eps*255)\n",
    "    x_adv = np.clip(x_adv, 0, 255.)\n",
    "    \n",
    "    return x_adv\n",
    "\n",
    "\n",
    "# https://github.com/ftramer/Excessive-Invariance\n",
    "# tries all rotation-translations of the input and returns the closest neighbor from each class\n",
    "def get_best_neighbors(x, y, all_NNs, grid):\n",
    "    xs = [shift(rotate(x, r, reshape=False), (tx, ty)).reshape(784) for (tx, ty, r) in grid]\n",
    "    xs = np.asarray(xs.copy())\n",
    "    \n",
    "    nns = []\n",
    "    y_nns = []\n",
    "    grids_nn = []\n",
    "    \n",
    "    # find a nearest neighbor in each class\n",
    "    for i in range(10):\n",
    "        if i != y:\n",
    "            X = X_train[Y_train == i]\n",
    "            Y = Y_train[Y_train == i]\n",
    "            distances, indices = all_NNs[i].kneighbors(xs, n_neighbors=1)\n",
    "\n",
    "            best = np.argmin(np.reshape(distances, -1))\n",
    "            best_idx = np.reshape(indices, -1)[best]\n",
    "            nns.append(X[best_idx])\n",
    "            y_nns.append(Y[best_idx])\n",
    "            \n",
    "            # store the inverse rotation+translation to be applied to the target\n",
    "            grids_nn.append(-np.asarray(grid[best]))\n",
    "    \n",
    "    return nns, y_nns, grids_nn\n",
    "\n",
    "\n",
    "# https://github.com/ftramer/Excessive-Invariance\n",
    "def generate_inv_adv_examples(epsilon_to_use, count):\n",
    "    import numpy as np\n",
    "    assert epsilon_to_use==0.3 or epsilon_to_use==0.4\n",
    "    \n",
    "    idxs=np.arange(0,count,1,dtype=int)\n",
    "\n",
    "    #  Load the MNIST data. 300 randomly chosen test point\n",
    "    assert len(idxs) == count\n",
    "    test_xs = X_test[idxs]\n",
    "    test_ys = Y_test[idxs]\n",
    "\n",
    "    # build a nearest neighbors classifier per class\n",
    "    N = 1\n",
    "    all_NNs = []\n",
    "\n",
    "    for i in range(10):\n",
    "        #Reshape to 1D (28*28=784)\n",
    "        X = X_train[Y_train == i].reshape(-1, 784)\n",
    "        nn = NearestNeighbors(n_neighbors=N)\n",
    "    \n",
    "        nn.fit(X)\n",
    "        all_NNs.append(nn)\n",
    "    # print(all_NNs)\n",
    "\n",
    "\n",
    "\n",
    "    # Rotation-translation parameters\n",
    "    limits = [3, 3, 30]\n",
    "    granularity = [5, 5, 31]\n",
    "    grid = list(product(*list(np.linspace(-l, l, num=g) for l, g in zip(limits, granularity))))\n",
    "\n",
    "\n",
    "\n",
    "    all_nns = []\n",
    "    all_y_nns = []\n",
    "    all_grids_nns = []\n",
    "\n",
    "    # find nearest neighbors for some test inputs (this takes a little while)\n",
    "    for i in range(len(idxs)):\n",
    "        if i % 10 == 0:\n",
    "            print(\"{}/{} done\".format(i, len(idxs)))\n",
    "        x = test_xs[i]\n",
    "        y = test_ys[i]\n",
    "\n",
    "        # find the nearest neighbors for each class, with the corresponding rotation and translation\n",
    "        nns, y_nns, grids_nns = get_best_neighbors(x, y, all_NNs, grid)\n",
    "        nn_advs = [shift(rotate(nn, r, reshape=False), (tx, ty)) for (nn, (tx, ty, r)) in zip(nns, grids_nns)]\n",
    "        all_nns.append(nn_advs)\n",
    "        all_y_nns.append(y_nns)\n",
    "        all_grids_nns.append(np.asarray(grids_nns))\n",
    "\n",
    "\n",
    "\n",
    "    # save everything!\n",
    "    np.save(\"data/invariance_examples_generation/X_test_{}.npy\".format(count), test_xs)\n",
    "    np.save(\"data/invariance_examples_generation/all_nns.npy\", np.asarray(all_nns))\n",
    "    np.save(\"data/invariance_examples_generation/all_y_nns.npy\", np.asarray(all_y_nns))\n",
    "    np.save(\"data/invariance_examples_generation/all_grids_nns.npy\", np.asarray(all_grids_nns))\n",
    "\n",
    "\n",
    "\n",
    "    # (X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "    all_nns=np.load(\"data/invariance_examples_generation/all_nns.npy\")\n",
    "    all_y_nns=np.load(\"data/invariance_examples_generation/all_y_nns.npy\")\n",
    "    all_grids_nns=np.load(\"data/invariance_examples_generation/all_grids_nns.npy\")\n",
    "    test_xs=np.load(\"data/invariance_examples_generation/X_test_{}.npy\".format(count))\n",
    "  \n",
    "    test_ys = y_test[idxs]\n",
    "\n",
    "    # manually chosen target classes for each source class\n",
    "    targets = {\n",
    "        0: [4, 6, 8, 9],\n",
    "        1: [4, 6, 7, 9],\n",
    "        2: [8],\n",
    "        3: [8],\n",
    "        4: [8, 9],\n",
    "        5: [3, 8],\n",
    "        6: [0],\n",
    "        7: [2, 3],\n",
    "        8: [3],\n",
    "        9: [3, 4, 5]\n",
    "    }\n",
    "\n",
    "    best_y_advs = []\n",
    "    best_targets = []\n",
    "    best_advs = []\n",
    "\n",
    "    for i in range(len(all_nns)):\n",
    "        x = test_xs[i]\n",
    "        y = test_ys[i]\n",
    "    \n",
    "        best_x_adv = None\n",
    "        best_nn_adv = None\n",
    "        amount_removed = []\n",
    "        amount_added = []\n",
    "        rot = []\n",
    "        best_y = None\n",
    "        min_removed = np.inf\n",
    "        for j in range(len(all_nns[i])):\n",
    "            nn_adv = all_nns[i][j]\n",
    "            y_nn = all_y_nns[i][j]\n",
    "            # print(\"NN ADV: {}\".format(np.shape(nn_adv)))\n",
    "            # print(\"X: {}\".format(np.shape(x)))\n",
    "            x_adv = linf_attack(x, nn_adv, epsilon_to_use)\n",
    "        \n",
    "            \n",
    "            # retain the target that required the least amount of pixels to be \"removed\"\n",
    "            curr_rot = np.abs(all_grids_nns[i][j][-1])\n",
    "            curr_removed = np.sum(np.abs(np.maximum(x/255. - x_adv/255., 0)))\n",
    "            \n",
    "            if y_nn in targets[y] and curr_removed < min_removed:\n",
    "                min_removed = curr_removed\n",
    "                best_y = y_nn\n",
    "                best_x_adv = x_adv\n",
    "                best_nn_adv = (nn_adv, y_nn)\n",
    "                    \n",
    "        best_targets.append(best_nn_adv)\n",
    "        best_advs.append(best_x_adv)\n",
    "        best_y_advs.append(best_y)\n",
    "        \n",
    "\n",
    "    \n",
    "    # if epsilon_to_use==0.3:\n",
    "    #     np.save(\"data/invariance_examples/epsilon_0.3/invariance-based_adversarial_examples\", best_advs)\n",
    "    #     np.save(\"data/invariance_examples/epsilon_0.3/invariance-based_adversarial_examples_new_labels\", best_y_advs)\n",
    "    # else:\n",
    "    #     np.save(\"data/invariance_examples/epsilon_0.4/invariance-based_adversarial_examples\", best_advs)\n",
    "    #     np.save(\"data/invariance_examples/epsilon_0.4/invariance-based_adversarial_examples_new_labels\", best_y_advs)\n",
    "    \n",
    "def ptb_training(ptb_acc_to_achieve, model_to_train, include_inv_training=False, inclusive_training=False, use_iterations=False, iterations=10):\n",
    "    if inclusive_training==True:\n",
    "        include_inv_training=False\n",
    "\n",
    "    inv_advs_to_train=np.load(\"data/invariance_examples/epsilon_0.3/invariance-based_adversarial_examples.npy\")\n",
    "    inv_labels_to_train=np.load(\"data/invariance_examples/epsilon_0.3/invariance-based_adversarial_examples_human_labels.npy\")\n",
    "    \n",
    "    earlystopping = callbacks.EarlyStopping(monitor =\"val_loss\", \n",
    "                                        mode =\"min\", patience = 1, \n",
    "                                       restore_best_weights = True)\n",
    "    #While ACCURACY\n",
    "    ptb_acc=0\n",
    "    i=0\n",
    "    y_axis=[]\n",
    "    x_axis_ptb=[]\n",
    "    x_axis_clean=[]\n",
    "    x_axis_inv=[]\n",
    "    if use_iterations==False:    \n",
    "        while ptb_acc<=ptb_acc_to_achieve:\n",
    "            res=test_model(model_to_train)\n",
    "            ptb_acc=res.get(\"ptb\").get(\"accuracy\")\n",
    "            clean_acc=res.get(\"clean\").get(\"accuracy\")\n",
    "            inv_acc=res.get(\"inv\").get(\"accuracy\")\n",
    "\n",
    "            i+=1\n",
    "            y_axis.append(i)\n",
    "            x_axis_ptb.append(ptb_acc)\n",
    "            x_axis_clean.append(clean_acc)\n",
    "            x_axis_inv.append(inv_acc)\n",
    "            fmodel=fb.models.tensorflow.TensorFlowModel(model_to_train, bounds=(0,1))   \n",
    "            x_batch,y_batch=next_batch(x_train,y_train, \"mnist\")\n",
    "            \n",
    "            x_batch_to_train = tf.convert_to_tensor(x_batch, dtype=tf.float32)\n",
    "            y_batch_to_train=tf.convert_to_tensor(y_batch, dtype=tf.int32)\n",
    "\n",
    "            #attack model    \n",
    "            _,advs, success=attack(fmodel, x_batch_to_train, y_batch_to_train, epsilons=epsilon) \n",
    "            success_rate=tf.keras.backend.get_value(success).mean(axis=-1).round(2)\n",
    "\n",
    "\n",
    "\n",
    "            if inclusive_training==True:\n",
    "                x=tf.keras.backend.get_value(advs)\n",
    "                x=x[:,:,:,0]\n",
    "\n",
    "                # perturbation based adversarial examples\n",
    "                x_training=x[0:int(len(x)*0.8)]\n",
    "                x_validation=x[int(len(x)*0.8):int(len(x))]\n",
    "                y_training=y_batch[0:int(len(x)*0.8)]\n",
    "                y_validation=y_batch[int(len(x)*0.8):int(len(x))]\n",
    "\n",
    "                # invariance based adversarial examples\n",
    "                x_inv,y_inv=next_batch(inv_advs_to_train,inv_labels_to_train, \"inv\")\n",
    "                x_inv_training=x_inv[0:int(len(x_inv)*0.8)]\n",
    "                x_inv_validation=x_inv[int(len(x_inv)*0.8):int(len(x_inv))]\n",
    "                y_inv_training=y_inv[0:int(len(y_inv)*0.8)]\n",
    "                y_inv_validation=y_inv[int(len(y_inv)*0.8):int(len(y_inv))]\n",
    "\n",
    "\n",
    "\n",
    "                # combine them into one array\n",
    "                x_training=np.append(x_training,x_inv_training, axis=0)\n",
    "                x_validation=np.append(x_validation,x_inv_validation, axis=0)\n",
    "                y_training=np.append(y_training,y_inv_training)\n",
    "                y_validation=np.append(y_validation,y_inv_validation)\n",
    "             \n",
    "               \n",
    "                \n",
    "                model_to_train.fit(x_training,to_categorical(y_training,num_classes=10),\n",
    "                    validation_data =(x_validation,to_categorical(y_validation, num_classes=10)),\n",
    "                    epochs=epochs,\n",
    "                    verbose=0,\n",
    "                    callbacks =[earlystopping])\n",
    "\n",
    "            else:         \n",
    "                #Retrain model with generated perturbation-based adversarial examples\n",
    "                #80% Training 20% Validation\n",
    "                x=tf.keras.backend.get_value(advs)\n",
    "                # print(\"Shape of x_training before reshape: {}\".format(np.shape(x)))\n",
    "                # print(\"Shape of x_training after reshape: {}\".format(np.shape(x)))\n",
    "                x_training=x[0:int(len(x)*0.8)]\n",
    "                x_validation=x[int(len(x)*0.8):int(len(x))]\n",
    "                y_training=y_batch[0:int(len(x)*0.8)]\n",
    "                y_validation=y_batch[int(len(x)*0.8):int(len(x))]\n",
    "                \n",
    "                model_to_train.fit(x_training,to_categorical(y_training,num_classes=10),\n",
    "                    validation_data =(x_validation,to_categorical(y_validation, num_classes=10)),\n",
    "                    epochs=epochs,\n",
    "                    verbose=0,\n",
    "                    callbacks =[earlystopping]\n",
    "                )\n",
    "\n",
    "                if include_inv_training==True:\n",
    "                    x_training=inv_advs_to_train[0:int(len(inv_advs_to_train)*0.8)]\n",
    "                    x_validation=inv_advs_to_train[int(len(inv_advs_to_train)*0.8):int(len(inv_advs_to_train))]\n",
    "                    y_training=inv_labels_to_train[0:int(len(inv_labels_to_train)*0.8)]\n",
    "                    y_validation=inv_labels_to_train[int(len(inv_labels_to_train)*0.8):int(len(inv_advs_to_train))]\n",
    "                    model_to_train.fit(x_training,to_categorical(y_training,num_classes=10),\n",
    "                        validation_data =(x_validation,to_categorical(y_validation, num_classes=10)),\n",
    "                        epochs=10,\n",
    "                        verbose=0,\n",
    "                        callbacks =[earlystopping]\n",
    "                    )\n",
    "\n",
    "\n",
    "       \n",
    "        \n",
    "\n",
    "            print(\"i: {} ptb acc: {}, inv_acc: {}\".format(i,ptb_acc, inv_acc))\n",
    "    else:\n",
    "        while i<iterations:\n",
    "            res=test_model(model_to_train)\n",
    "            ptb_acc=res.get(\"ptb\").get(\"accuracy\")\n",
    "            clean_acc=res.get(\"clean\").get(\"accuracy\")\n",
    "            inv_acc=res.get(\"inv\").get(\"accuracy\")\n",
    "\n",
    "            i+=1\n",
    "            y_axis.append(i)\n",
    "            x_axis_ptb.append(ptb_acc)\n",
    "            x_axis_clean.append(clean_acc)\n",
    "            x_axis_inv.append(inv_acc)\n",
    "            fmodel=fb.models.tensorflow.TensorFlowModel(model_to_train, bounds=(0,1))   \n",
    "            x_batch,y_batch=next_batch(x_train,y_train, \"mnist\")\n",
    "            \n",
    "            x_batch_to_train = tf.convert_to_tensor(x_batch, dtype=tf.float32)\n",
    "            y_batch_to_train=tf.convert_to_tensor(y_batch, dtype=tf.int32)\n",
    "\n",
    "            #attack model    \n",
    "            _,advs, success=attack(fmodel, x_batch_to_train, y_batch_to_train, epsilons=epsilon) \n",
    "            success_rate=tf.keras.backend.get_value(success).mean(axis=-1).round(2)\n",
    "            \n",
    "            if inclusive_training==True:\n",
    "                x=tf.keras.backend.get_value(advs)\n",
    "                x=x[:,:,:,0]\n",
    "\n",
    "                # perturbation based adversarial examples\n",
    "                x_training=x[0:int(len(x)*0.8)]\n",
    "                x_validation=x[int(len(x)*0.8):int(len(x))]\n",
    "                y_training=y_batch[0:int(len(x)*0.8)]\n",
    "                y_validation=y_batch[int(len(x)*0.8):int(len(x))]\n",
    "\n",
    "                # invariance based adversarial examples\n",
    "                x_inv,y_inv=next_batch(inv_advs_to_train,inv_labels_to_train, \"inv\")\n",
    "                x_inv_training=x_inv[0:int(len(x_inv)*0.8)]\n",
    "                x_inv_validation=x_inv[int(len(x_inv)*0.8):int(len(x_inv))]\n",
    "                y_inv_training=y_inv[0:int(len(y_inv)*0.8)]\n",
    "                y_inv_validation=y_inv[int(len(y_inv)*0.8):int(len(y_inv))]\n",
    "\n",
    "\n",
    "\n",
    "                # combine them into one array\n",
    "                x_training=np.append(x_training,x_inv_training, axis=0)\n",
    "                x_validation=np.append(x_validation,x_inv_validation, axis=0)\n",
    "                y_training=np.append(y_training,y_inv_training)\n",
    "                y_validation=np.append(y_validation,y_inv_validation)\n",
    "             \n",
    "               \n",
    "                \n",
    "                model_to_train.fit(x_training,to_categorical(y_training,num_classes=10),\n",
    "                    validation_data =(x_validation,to_categorical(y_validation, num_classes=10)),\n",
    "                    epochs=epochs,\n",
    "                    verbose=0,\n",
    "                    callbacks =[earlystopping]\n",
    "                )\n",
    "\n",
    "            else:         \n",
    "                #Retrain model with generated perturbation-based adversarial examples\n",
    "                #80% Training 20% Validation\n",
    "                x=tf.keras.backend.get_value(advs)\n",
    "                # print(\"Shape of x_training before reshape: {}\".format(np.shape(x)))\n",
    "                # print(\"Shape of x_training after reshape: {}\".format(np.shape(x)))\n",
    "                x_training=x[0:int(len(x)*0.8)]\n",
    "                x_validation=x[int(len(x)*0.8):int(len(x))]\n",
    "                y_training=y_batch[0:int(len(x)*0.8)]\n",
    "                y_validation=y_batch[int(len(x)*0.8):int(len(x))]\n",
    "                \n",
    "                model_to_train.fit(x_training,to_categorical(y_training,num_classes=10),\n",
    "                    validation_data =(x_validation,to_categorical(y_validation, num_classes=10)),\n",
    "                    epochs=epochs,\n",
    "                    verbose=0,\n",
    "                    callbacks =[earlystopping]\n",
    "                )\n",
    "                if include_inv_training==True:\n",
    "                    x_training=inv_advs_to_train[0:int(len(inv_advs_to_train)*0.8)]\n",
    "                    x_validation=inv_advs_to_train[int(len(inv_advs_to_train)*0.8):int(len(inv_advs_to_train))]\n",
    "                    y_training=inv_labels_to_train[0:int(len(inv_labels_to_train)*0.8)]\n",
    "                    y_validation=inv_labels_to_train[int(len(inv_labels_to_train)*0.8):int(len(inv_advs_to_train))]\n",
    "                    model_to_train.fit(x_training,to_categorical(y_training,num_classes=10),\n",
    "                        validation_data =(x_validation,to_categorical(y_validation, num_classes=10)),\n",
    "                        epochs=10,\n",
    "                        verbose=0,\n",
    "                        callbacks =[earlystopping]\n",
    "                    )\n",
    "        \n",
    "            print(\"i: {} ptb acc: {}, inv_acc: {}\".format(i,ptb_acc, inv_acc))\n",
    "    plt.plot( y_axis, x_axis_inv, label = \"INV\")\n",
    "    plt.plot( y_axis, x_axis_clean, label = \"Clean\")\n",
    "    plt.plot( y_axis, x_axis_ptb,label = \"PTB\")\n",
    "    plt.xlabel('Iterationen')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return {\n",
    "        \"model\": model_to_train,\n",
    "        \"clean\":{ \"accuracy\": x_axis_clean},\n",
    "        \"ptb\":{\"accuracy\":x_axis_ptb},\n",
    "        \"inv\":{\"accuracy\":x_axis_inv},\n",
    "        \n",
    "\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create/train vanilla model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_vanilla_model().save(\"models/vanilla_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attack Vanilla Model and Retrain with Perturbation-Based Adversarial Examples iteratively\n",
    "Result is ptb_trained_model\n",
    "\n",
    "Takes a few minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get Model\n",
    "model=load_model(\"models/vanilla_model\")\n",
    "\n",
    "ptb_acc_to_achieve=0.88\n",
    "model,ptb_acc=ptb_training(ptb_acc_to_achieve, model, use_iterations=True, iterations=100)\n",
    "model.save(\"models/ptb_trained_model_{}_ptb_accuracy\".format(ptb_acc)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot PTB Adversarial Training graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_array(array):\n",
    "    filtered=[]\n",
    "    for i in range(len(array)):\n",
    "        if i%10==0:\n",
    "            filtered.append(array[i])\n",
    "    return filtered\n",
    "\n",
    "\n",
    "\n",
    "y=np.load(\"data/ptb_training/iteration_count_arr.npy\")\n",
    "clean_arr=np.load(\"data/ptb_training/clean_accuracy_arr.npy\")\n",
    "ptb_arr=np.load(\"data/ptb_training/ptb_accuracy_arr.npy\")\n",
    "plt.plot( y, clean_arr, label = \"Clean\")\n",
    "plt.plot( y, ptb_arr,label = \"PTB\")\n",
    "plt.xlabel('Iterationen')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "new_arr_x_ptb=filter_array(ptb_arr)\n",
    "new_arr_x_clean=filter_array(clean_arr)\n",
    "new_arr_y=filter_array(y)\n",
    "\n",
    "\n",
    "plt.plot( new_arr_y, new_arr_x_clean, label = \"Clean\")\n",
    "plt.plot( new_arr_y, new_arr_x_ptb,label = \"PTB\")\n",
    "plt.xlabel('Iterationen')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Max accuracy against PTB: {}\".format(np.max(ptb_arr)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate INV-Based ADV-Examples \n",
    "Code is from https://github.com/ftramer/Excessive-Invariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# generate_inv_adv_examples(0.3,500)\n",
    "# generate_inv_adv_examples(0.4,500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot INV-Based ADV-Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_advs_to_train=np.load(\"data/invariance_examples/epsilon_0.3/invariance-based_adversarial_examples.npy\")\n",
    "inv_labels_to_train=np.load(\"data/invariance_examples/epsilon_0.3/invariance-based_adversarial_examples_new_labels.npy\")\n",
    "\n",
    "print(\"----------EPSILON=0.3----------\")\n",
    "fig, axes = plt.subplots(50,10, figsize=(1.5*10,2*50))\n",
    "for i in range(500):\n",
    "    ax = axes[i//10,i%10]\n",
    "    ax.imshow(inv_advs_to_train[i], cmap='gray')\n",
    "    # ax.set_title('Label: {}'.format(inv_labels_to_train[i]))\n",
    "    ax.set_title('Count: {}'.format(i))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "inv_advs_to_train=np.load(\"data/invariance_examples/epsilon_0.4/invariance-based_adversarial_examples.npy\")\n",
    "inv_labels_to_train=np.load(\"data/invariance_examples/epsilon_0.4/invariance-based_adversarial_examples_new_labels.npy\")\n",
    "\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"----------EPSILON=0.4----------\")\n",
    "fig, axes = plt.subplots(50,10, figsize=(1.5*10,2*50))\n",
    "for i in range(500):\n",
    "    ax = axes[i//10,i%10]\n",
    "    ax.imshow(inv_advs_to_train[i], cmap='gray')\n",
    "    # ax.set_title('Label: {}'.format(inv_labels_to_train[i]))\n",
    "    ax.set_title('Count: {}'.format(i))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Erster Durchlauf (Anzahl an Invariance-Based Adversarial Examples beim Trainieren variiert. Immer die neuen Labels verÃ¤ndern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epsilon\n",
    "epsilon=0.3\n",
    "\n",
    "# c\n",
    "\n",
    "c=[]\n",
    "i=500\n",
    "j=5\n",
    "while j<=i:\n",
    "    c.append(j)\n",
    "    j+=5\n",
    "\n",
    "vanilla_model=load_model(\"models/vanilla_model\")\n",
    "\n",
    "# m=l_infinity_PGD\n",
    "# a=88.9\n",
    "ptb_trained_model=load_model(\"models/ptb_trained_model_0.889_ptb_accuracy_PGD\")\n",
    "\n",
    "# Invariance-Based Adversarial Examples to train, use ONLY THE NEW LABELS\n",
    "inv_advs_to_train=np.load(\"data/invariance_examples/epsilon_0.3/invariance-based_adversarial_examples.npy\")\n",
    "inv_labels_to_train=np.load(\"data/invariance_examples/epsilon_0.3/invariance-based_adversarial_examples_new_labels.npy\")\n",
    "\n",
    "\n",
    "\n",
    "# Initialize writing results to csv\n",
    "handler_inv_trained = open('data/results/erster_durchlauf/inv_trained.csv', 'w',encoding='UTF8',newline='')\n",
    "writer_inv_trained = csv.writer(handler_inv_trained)\n",
    "writer_inv_trained.writerow([\"c\",\"clean_acc\",\"ptb_acc\", \"inv_acc\", \"clean_loss\", \"ptb_loss\", \"inv_loss\", \"inv_success_rate\" ])\n",
    "\n",
    "\n",
    "handler_ptb_inv_trained = open('data/results/erster_durchlauf/ptb_inv_trained.csv', 'w',encoding='UTF8',newline='')\n",
    "writer_ptb_inv_trained = csv.writer(handler_ptb_inv_trained)\n",
    "writer_ptb_inv_trained.writerow([\"c\",\"clean_acc\",\"ptb_acc\", \"inv_acc\", \"clean_loss\", \"ptb_loss\", \"inv_loss\", \"inv_success_rate\" ])\n",
    "\n",
    "\n",
    "\n",
    "initial_results_vanilla=test_model(vanilla_model)\n",
    "initial_results_ptb=test_model(ptb_trained_model)\n",
    "\n",
    "data=[0,initial_results_vanilla.get(\"clean\").get(\"accuracy\"),\n",
    "    initial_results_vanilla.get(\"ptb\").get(\"accuracy\"),\n",
    "    initial_results_vanilla.get(\"inv\").get(\"accuracy\"),\n",
    "    initial_results_vanilla.get(\"clean\").get(\"loss\"),\n",
    "    initial_results_vanilla.get(\"ptb\").get(\"loss\"),\n",
    "    initial_results_vanilla.get(\"inv\").get(\"loss\"),\n",
    "    initial_results_vanilla.get(\"inv_success_rate\"),\n",
    "    ]\n",
    "\n",
    "writer_inv_trained.writerow(data)\n",
    "\n",
    "data=[0,initial_results_ptb.get(\"clean\").get(\"accuracy\"),\n",
    "    initial_results_ptb.get(\"ptb\").get(\"accuracy\"),\n",
    "    initial_results_ptb.get(\"inv\").get(\"accuracy\"),\n",
    "    initial_results_ptb.get(\"clean\").get(\"loss\"),\n",
    "    initial_results_ptb.get(\"ptb\").get(\"loss\"),\n",
    "    initial_results_ptb.get(\"inv\").get(\"loss\"),\n",
    "    initial_results_ptb.get(\"inv_success_rate\"),\n",
    "    ]\n",
    "\n",
    "writer_ptb_inv_trained.writerow(data)\n",
    "\n",
    "\n",
    "print(\"Initial results from Vanilla Model: {}\".format(initial_results_vanilla))\n",
    "print(\"Initial results from PTB-Trained Model: {}\".format(initial_results_ptb))\n",
    "\n",
    "\n",
    "results_inv_trained=[]\n",
    "results_ptb_inv_trained=[]\n",
    "for i in range(len(c)):\n",
    "    print(\"Training with {} examples...\".format(c[i]))\n",
    "\n",
    "    vanilla_model.fit(inv_advs_to_train[0:c[i]],to_categorical(inv_labels_to_train[0:c[i]],num_classes=10),\n",
    "    epochs=10,\n",
    "    verbose=0)\n",
    "    \n",
    "\n",
    "    res=test_model(vanilla_model)\n",
    "    results_inv_trained.append(res)\n",
    "    data=[c[i],res.get(\"clean\").get(\"accuracy\"),\n",
    "    res.get(\"ptb\").get(\"accuracy\"),\n",
    "    res.get(\"inv\").get(\"accuracy\"),\n",
    "    res.get(\"clean\").get(\"loss\"),\n",
    "    res.get(\"ptb\").get(\"loss\"),\n",
    "    res.get(\"inv\").get(\"loss\"),\n",
    "    res.get(\"inv_success_rate\"),\n",
    "    ]\n",
    "    # write to csv file\n",
    "    writer_inv_trained.writerow(data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ptb_trained_model.fit(inv_advs_to_train[0:c[i]],to_categorical(inv_labels_to_train[0:c[i]],num_classes=10),\n",
    "    epochs=10,\n",
    "    verbose=0)\n",
    "\n",
    "    res=test_model(ptb_trained_model)\n",
    "    results_ptb_inv_trained.append(res)\n",
    "    data=[c[i],res.get(\"clean\").get(\"accuracy\"),\n",
    "    res.get(\"ptb\").get(\"accuracy\"),\n",
    "    res.get(\"inv\").get(\"accuracy\"),\n",
    "    res.get(\"clean\").get(\"loss\"),\n",
    "    res.get(\"ptb\").get(\"loss\"),\n",
    "    res.get(\"inv\").get(\"loss\"),\n",
    "    res.get(\"inv_success_rate\"),\n",
    "    ]\n",
    "\n",
    "    #write to csv file\n",
    "    writer_ptb_inv_trained.writerow(data)\n",
    "\n",
    "    #reload models...\n",
    "    vanilla_model=load_model(\"models/vanilla_model\")\n",
    "    ptb_trained_model=load_model(\"models/ptb_trained_model_0.889_ptb_accuracy_PGD\")\n",
    "handler_ptb_inv_trained.close()\n",
    "handler_inv_trained.close()\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"----------Results INV-Trained Model----------\")\n",
    "i=0\n",
    "for entry in results_inv_trained:\n",
    "    print(\"Clean accuracy INV_trained with {} examples: {}\".format(c[i],entry.get(\"clean\").get(\"accuracy\")))\n",
    "    i+=1\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"----------Results PTB-INV-Trained Model----------\")\n",
    "i=0\n",
    "for entry in results_ptb_inv_trained:\n",
    "    print(\"Clean accuracy PTB-INV_trained with {} examples: {}\".format(c[i],entry.get(\"clean\").get(\"accuracy\")))\n",
    "    i+=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ersten Durchlauf evaluieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "handler_inv_trained = open('data/results/erster_durchlauf/inv_trained.csv', 'r')\n",
    "reader_inv_trained = csv.DictReader(handler_inv_trained)\n",
    "\n",
    "handler_ptb_inv_trained = open('data/results/erster_durchlauf/ptb_inv_trained.csv', 'r')\n",
    "reader_ptb_inv_trained = csv.DictReader(handler_ptb_inv_trained)\n",
    "\n",
    "\n",
    "\n",
    "inv_trained_clean=[]\n",
    "inv_trained_ptb=[]\n",
    "inv_trained_inv=[]\n",
    "\n",
    "ptb_inv_trained_clean=[]\n",
    "ptb_inv_trained_ptb=[]\n",
    "ptb_inv_trained_inv=[]\n",
    "\n",
    "\n",
    "line_count = 0\n",
    "for row in reader_inv_trained:\n",
    "    \n",
    "    inv_trained_clean.append(float(row[\"clean_acc\"]))        \n",
    "    inv_trained_ptb.append(float(row[\"ptb_acc\"]))\n",
    "    inv_trained_inv.append(float(row[\"inv_acc\"]))\n",
    "    line_count+=1\n",
    "\n",
    "line_count = 0\n",
    "for row in reader_ptb_inv_trained:\n",
    "    \n",
    "    ptb_inv_trained_clean.append(float(row[\"clean_acc\"]))\n",
    "    ptb_inv_trained_ptb.append(float(row[\"ptb_acc\"]))\n",
    "    ptb_inv_trained_inv.append(float(row[\"inv_acc\"]))\n",
    "    line_count+=1\n",
    "\n",
    "\n",
    "y=[]\n",
    "i=500\n",
    "j=0\n",
    "while j<=500:\n",
    "    y.append(j)\n",
    "    j+=5\n",
    "\n",
    "\n",
    "print(\"INV-Trained\")  \n",
    "plt.plot( y, inv_trained_clean, label = \"Clean\")\n",
    "plt.plot( y, inv_trained_ptb,label = \"PTB\")\n",
    "plt.plot( y, inv_trained_inv,label = \"INV\")\n",
    "plt.xlabel('c')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"PTB-INV-Trained\")\n",
    "plt.plot( y, ptb_inv_trained_clean, label = \"Clean\")\n",
    "plt.plot( y, ptb_inv_trained_ptb,label = \"PTB\")\n",
    "plt.plot( y, ptb_inv_trained_inv,label = \"INV\")\n",
    "plt.xlabel('c')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "inv_trained_clean={\n",
    "    \"initial\": inv_trained_clean[0],\n",
    "    \"mean\":np.mean(inv_trained_clean),\n",
    "    \"min\":np.min(inv_trained_clean),\n",
    "    \"max\":np.max(inv_trained_clean)\n",
    "}\n",
    "\n",
    "inv_trained_ptb={\n",
    "    \"initial\": inv_trained_ptb[0],\n",
    "    \"mean\":np.mean(inv_trained_ptb),\n",
    "    \"min\":np.min(inv_trained_ptb),\n",
    "    \"max\":np.max(inv_trained_ptb)\n",
    "}\n",
    "\n",
    "inv_trained_inv={\n",
    "    \"initial\": inv_trained_inv[0],\n",
    "    \"mean\":np.mean(inv_trained_inv),\n",
    "    \"min\":np.min(inv_trained_inv),\n",
    "    \"max\":np.max(inv_trained_inv)\n",
    "}\n",
    "\n",
    "\n",
    "ptb_inv_trained_clean={\n",
    "    \"initial\": ptb_inv_trained_clean[0],\n",
    "    \"mean\":np.mean(ptb_inv_trained_clean),\n",
    "    \"min\":np.min(ptb_inv_trained_clean),\n",
    "    \"max\":np.max(ptb_inv_trained_clean)\n",
    "}\n",
    "\n",
    "ptb_inv_trained_ptb={\n",
    "    \"initial\": ptb_inv_trained_ptb[0],\n",
    "    \"mean\":np.mean(ptb_inv_trained_ptb),\n",
    "    \"min\":np.min(ptb_inv_trained_ptb),\n",
    "    \"max\":np.max(ptb_inv_trained_ptb)\n",
    "}\n",
    "\n",
    "ptb_inv_trained_inv={\n",
    "    \"initial\": ptb_inv_trained_inv[0],\n",
    "    \"mean\":np.mean(ptb_inv_trained_inv),\n",
    "    \"min\":np.min(ptb_inv_trained_inv),\n",
    "    \"max\":np.max(ptb_inv_trained_inv)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "print(\"INV_TRAINED\")\n",
    "print()\n",
    "print(\"INV-TRAINED CLEAN: initial: {}, min: {}, max: {}, mean: {}\".format(inv_trained_clean.get(\"initial\"),inv_trained_clean.get(\"min\"),inv_trained_clean.get(\"max\"),inv_trained_clean.get(\"mean\")))\n",
    "print(\"INV-TRAINED PTB: initial: {}, min: {}, max: {}, mean: {}\".format(inv_trained_ptb.get(\"initial\"),inv_trained_ptb.get(\"min\"),inv_trained_ptb.get(\"max\"),inv_trained_ptb.get(\"mean\")))\n",
    "print(\"INV-TRAINED INV: initial: {}, min: {}, max: {}, mean: {}\".format(inv_trained_inv.get(\"initial\"),inv_trained_inv.get(\"min\"),inv_trained_inv.get(\"max\"),inv_trained_inv.get(\"mean\")))\n",
    "print()\n",
    "print()\n",
    "print(\"PTB-INV_TRAINED\")\n",
    "print()\n",
    "print(\"PTB_INV-TRAINED CLEAN: initial: {}, min: {}, max: {}, mean: {}\".format(ptb_inv_trained_clean.get(\"initial\"),ptb_inv_trained_clean.get(\"min\"),ptb_inv_trained_clean.get(\"max\"),ptb_inv_trained_clean.get(\"mean\")))\n",
    "print(\"PTB_INV-TRAINED PTB: initial: {}, min: {}, max: {}, mean: {}\".format(ptb_inv_trained_ptb.get(\"initial\"),ptb_inv_trained_ptb.get(\"min\"),ptb_inv_trained_ptb.get(\"max\"),ptb_inv_trained_ptb.get(\"mean\")))\n",
    "print(\"PTB_INV-TRAINED INV: initial: {}, min: {}, max: {}, mean: {}\".format(ptb_inv_trained_inv.get(\"initial\"),ptb_inv_trained_inv.get(\"min\"),ptb_inv_trained_inv.get(\"max\"),ptb_inv_trained_inv.get(\"mean\")))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zweiter Durchlauf (Dasselbe wie beim ersten Durchlauf mit dem Unterschied, dass die Labels beim Retrainieren mit Invariance-Based Adversarial Examples von zehn Personen bestimmt wurden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epsilon\n",
    "epsilon=0.3\n",
    "\n",
    "# c\n",
    "c=[]\n",
    "i=500\n",
    "j=5\n",
    "while j<=i:\n",
    "    c.append(j)\n",
    "    j+=5\n",
    "\n",
    "\n",
    "vanilla_model=load_model(\"models/vanilla_model\")\n",
    "\n",
    "# m=l_infinity_PGD\n",
    "# a=88.9\n",
    "ptb_trained_model=load_model(\"models/ptb_trained_model_0.889_ptb_accuracy_PGD\")\n",
    "\n",
    "# Invariance-Based Adversarial Examples to train, use ONLY THE NEW LABELS\n",
    "inv_advs_to_train=np.load(\"data/invariance_examples/epsilon_0.3/invariance-based_adversarial_examples.npy\")\n",
    "inv_labels_to_train=np.load(\"data/invariance_examples/epsilon_0.3/invariance-based_adversarial_examples_human_labels.npy\")\n",
    "\n",
    "print(np.shape(inv_labels_to_train))\n",
    "\n",
    "# Initialize writing results to csv\n",
    "handler_inv_trained = open('data/results/zweiter_durchlauf/inv_trained.csv', 'w',encoding='UTF8',newline='')\n",
    "writer_inv_trained = csv.writer(handler_inv_trained)\n",
    "writer_inv_trained.writerow([\"c\",\"clean_acc\",\"ptb_acc\", \"inv_acc\", \"clean_loss\", \"ptb_loss\", \"inv_loss\", \"inv_success_rate\" ])\n",
    "\n",
    "\n",
    "handler_ptb_inv_trained = open('data/results/zweiter_durchlauf/ptb_inv_trained.csv', 'w',encoding='UTF8',newline='')\n",
    "writer_ptb_inv_trained = csv.writer(handler_ptb_inv_trained)\n",
    "writer_ptb_inv_trained.writerow([\"c\",\"clean_acc\",\"ptb_acc\", \"inv_acc\", \"clean_loss\", \"ptb_loss\", \"inv_loss\", \"inv_success_rate\" ])\n",
    "\n",
    "initial_results_vanilla=test_model(vanilla_model)\n",
    "initial_results_ptb=test_model(ptb_trained_model)\n",
    "\n",
    "data=[0,initial_results_vanilla.get(\"clean\").get(\"accuracy\"),\n",
    "    initial_results_vanilla.get(\"ptb\").get(\"accuracy\"),\n",
    "    initial_results_vanilla.get(\"inv\").get(\"accuracy\"),\n",
    "    initial_results_vanilla.get(\"clean\").get(\"loss\"),\n",
    "    initial_results_vanilla.get(\"ptb\").get(\"loss\"),\n",
    "    initial_results_vanilla.get(\"inv\").get(\"loss\"),\n",
    "    initial_results_vanilla.get(\"inv_success_rate\"),\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "writer_inv_trained.writerow(data)\n",
    "\n",
    "data=[0,initial_results_ptb.get(\"clean\").get(\"accuracy\"),\n",
    "    initial_results_ptb.get(\"ptb\").get(\"accuracy\"),\n",
    "    initial_results_ptb.get(\"inv\").get(\"accuracy\"),\n",
    "    initial_results_ptb.get(\"clean\").get(\"loss\"),\n",
    "    initial_results_ptb.get(\"ptb\").get(\"loss\"),\n",
    "    initial_results_ptb.get(\"inv\").get(\"loss\"),\n",
    "    initial_results_ptb.get(\"inv_success_rate\"),\n",
    "    ]\n",
    "\n",
    "writer_ptb_inv_trained.writerow(data)\n",
    "\n",
    "print(\"Initial results from Vanilla Model: {}\".format(initial_results_vanilla))\n",
    "print(\"Initial results from PTB-Trained Model: {}\".format(initial_results_ptb))\n",
    "\n",
    "\n",
    "results_inv_trained=[]\n",
    "results_ptb_inv_trained=[]\n",
    "for i in range(len(c)):\n",
    "    print(\"Training with {} examples...\".format(c[i]))\n",
    "\n",
    "    vanilla_model.fit(inv_advs_to_train[0:c[i]],to_categorical(inv_labels_to_train[0:c[i]],num_classes=10),\n",
    "    epochs=10,\n",
    "    verbose=0)\n",
    "    \n",
    "\n",
    "    res=test_model(vanilla_model)\n",
    "    results_inv_trained.append(res)\n",
    "    data=[c[i],res.get(\"clean\").get(\"accuracy\"),\n",
    "    res.get(\"ptb\").get(\"accuracy\"),\n",
    "    res.get(\"inv\").get(\"accuracy\"),\n",
    "    res.get(\"clean\").get(\"loss\"),\n",
    "    res.get(\"ptb\").get(\"loss\"),\n",
    "    res.get(\"inv\").get(\"loss\"),\n",
    "    res.get(\"inv_success_rate\"),\n",
    "    ]\n",
    "    # write to csv file\n",
    "    writer_inv_trained.writerow(data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ptb_trained_model.fit(inv_advs_to_train[0:c[i]],to_categorical(inv_labels_to_train[0:c[i]],num_classes=10),\n",
    "    epochs=10,\n",
    "    verbose=0)\n",
    "\n",
    "    res=test_model(ptb_trained_model)\n",
    "    results_ptb_inv_trained.append(res)\n",
    "    data=[c[i],res.get(\"clean\").get(\"accuracy\"),\n",
    "    res.get(\"ptb\").get(\"accuracy\"),\n",
    "    res.get(\"inv\").get(\"accuracy\"),\n",
    "    res.get(\"clean\").get(\"loss\"),\n",
    "    res.get(\"ptb\").get(\"loss\"),\n",
    "    res.get(\"inv\").get(\"loss\"),\n",
    "    res.get(\"inv_success_rate\"),\n",
    "    ]\n",
    "\n",
    "    #write to csv file\n",
    "    writer_ptb_inv_trained.writerow(data)\n",
    "\n",
    "    #reload models...\n",
    "    vanilla_model=load_model(\"models/vanilla_model\")\n",
    "    ptb_trained_model=load_model(\"models/ptb_trained_model_0.889_ptb_accuracy_PGD\")\n",
    "\n",
    "handler_ptb_inv_trained.close()\n",
    "handler_inv_trained.close()\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"----------Results INV-Trained Model----------\")\n",
    "i=0\n",
    "for entry in results_inv_trained:\n",
    "    print(\"Clean accuracy INV_trained with {} examples: {}\".format(c[i],entry.get(\"clean\").get(\"accuracy\")))\n",
    "    i+=1\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"----------Results PTB-INV-Trained Model----------\")\n",
    "i=0\n",
    "for entry in results_ptb_inv_trained:\n",
    "    print(\"Clean accuracy PTB-INV_trained with {} examples: {}\".format(c[i],entry.get(\"clean\").get(\"accuracy\")))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zweiten Durchlauf evaluieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "handler_inv_trained = open('data/results/zweiter_durchlauf/inv_trained.csv', 'r')\n",
    "reader_inv_trained = csv.DictReader(handler_inv_trained)\n",
    "\n",
    "handler_ptb_inv_trained = open('data/results/zweiter_durchlauf/ptb_inv_trained.csv', 'r')\n",
    "reader_ptb_inv_trained = csv.DictReader(handler_ptb_inv_trained)\n",
    "\n",
    "\n",
    "\n",
    "inv_trained_clean=[]\n",
    "inv_trained_ptb=[]\n",
    "inv_trained_inv=[]\n",
    "\n",
    "ptb_inv_trained_clean=[]\n",
    "ptb_inv_trained_ptb=[]\n",
    "ptb_inv_trained_inv=[]\n",
    "\n",
    "\n",
    "line_count = 0\n",
    "for row in reader_inv_trained:\n",
    "    \n",
    "    inv_trained_clean.append(float(row[\"clean_acc\"]))        \n",
    "    inv_trained_ptb.append(float(row[\"ptb_acc\"]))\n",
    "    inv_trained_inv.append(float(row[\"inv_acc\"]))\n",
    "    line_count+=1\n",
    "\n",
    "line_count = 0\n",
    "for row in reader_ptb_inv_trained:\n",
    "    \n",
    "    ptb_inv_trained_clean.append(float(row[\"clean_acc\"]))\n",
    "    ptb_inv_trained_ptb.append(float(row[\"ptb_acc\"]))\n",
    "    ptb_inv_trained_inv.append(float(row[\"inv_acc\"]))\n",
    "    line_count+=1\n",
    "\n",
    "\n",
    "y=[]\n",
    "i=500\n",
    "j=0\n",
    "while j<=500:\n",
    "    y.append(j)\n",
    "    j+=5\n",
    "\n",
    "print(\"INV-Trained\")  \n",
    "plt.plot( y, inv_trained_clean, label = \"Clean\")\n",
    "plt.plot( y, inv_trained_ptb,label = \"PTB\")\n",
    "plt.plot( y, inv_trained_inv,label = \"INV\")\n",
    "plt.xlabel('c')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"PTB-INV-Trained\")\n",
    "plt.plot( y, ptb_inv_trained_clean, label = \"Clean\")\n",
    "plt.plot( y, ptb_inv_trained_ptb,label = \"PTB\")\n",
    "plt.plot( y, ptb_inv_trained_inv,label = \"INV\")\n",
    "plt.xlabel('c')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "inv_trained_clean={\n",
    "    \"initial\": inv_trained_clean[0],\n",
    "    \"mean\":np.mean(inv_trained_clean),\n",
    "    \"min\":np.min(inv_trained_clean),\n",
    "    \"max\":np.max(inv_trained_clean)\n",
    "}\n",
    "\n",
    "inv_trained_ptb={\n",
    "    \"initial\": inv_trained_ptb[0],\n",
    "    \"mean\":np.mean(inv_trained_ptb),\n",
    "    \"min\":np.min(inv_trained_ptb),\n",
    "    \"max\":np.max(inv_trained_ptb)\n",
    "}\n",
    "\n",
    "inv_trained_inv={\n",
    "    \"initial\": inv_trained_inv[0],\n",
    "    \"mean\":np.mean(inv_trained_inv),\n",
    "    \"min\":np.min(inv_trained_inv),\n",
    "    \"max\":np.max(inv_trained_inv)\n",
    "}\n",
    "\n",
    "\n",
    "ptb_inv_trained_clean={\n",
    "    \"initial\": ptb_inv_trained_clean[0],\n",
    "    \"mean\":np.mean(ptb_inv_trained_clean),\n",
    "    \"min\":np.min(ptb_inv_trained_clean),\n",
    "    \"max\":np.max(ptb_inv_trained_clean)\n",
    "}\n",
    "\n",
    "ptb_inv_trained_ptb={\n",
    "    \"initial\": ptb_inv_trained_ptb[0],\n",
    "    \"mean\":np.mean(ptb_inv_trained_ptb),\n",
    "    \"min\":np.min(ptb_inv_trained_ptb),\n",
    "    \"max\":np.max(ptb_inv_trained_ptb)\n",
    "}\n",
    "\n",
    "ptb_inv_trained_inv={\n",
    "    \"initial\": ptb_inv_trained_inv[0],\n",
    "    \"mean\":np.mean(ptb_inv_trained_inv),\n",
    "    \"min\":np.min(ptb_inv_trained_inv),\n",
    "    \"max\":np.max(ptb_inv_trained_inv)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"INV_TRAINED\")\n",
    "print()\n",
    "print(\"INV-TRAINED CLEAN: initial: {}, min: {}, max: {}, mean: {}\".format(inv_trained_clean.get(\"initial\"),inv_trained_clean.get(\"min\"),inv_trained_clean.get(\"max\"),inv_trained_clean.get(\"mean\")))\n",
    "print(\"INV-TRAINED PTB: initial: {}, min: {}, max: {}, mean: {}\".format(inv_trained_ptb.get(\"initial\"),inv_trained_ptb.get(\"min\"),inv_trained_ptb.get(\"max\"),inv_trained_ptb.get(\"mean\")))\n",
    "print(\"INV-TRAINED INV: initial: {}, min: {}, max: {}, mean: {}\".format(inv_trained_inv.get(\"initial\"),inv_trained_inv.get(\"min\"),inv_trained_inv.get(\"max\"),inv_trained_inv.get(\"mean\")))\n",
    "print()\n",
    "print()\n",
    "print(\"PTB-INV_TRAINED\")\n",
    "print()\n",
    "print(\"PTB_INV-TRAINED CLEAN: initial: {}, min: {}, max: {}, mean: {}\".format(ptb_inv_trained_clean.get(\"initial\"),ptb_inv_trained_clean.get(\"min\"),ptb_inv_trained_clean.get(\"max\"),ptb_inv_trained_clean.get(\"mean\")))\n",
    "print(\"PTB_INV-TRAINED PTB: initial: {}, min: {}, max: {}, mean: {}\".format(ptb_inv_trained_ptb.get(\"initial\"),ptb_inv_trained_ptb.get(\"min\"),ptb_inv_trained_ptb.get(\"max\"),ptb_inv_trained_ptb.get(\"mean\")))\n",
    "print(\"PTB_INV-TRAINED INV: initial: {}, min: {}, max: {}, mean: {}\".format(ptb_inv_trained_inv.get(\"initial\"),ptb_inv_trained_inv.get(\"min\"),ptb_inv_trained_inv.get(\"max\"),ptb_inv_trained_inv.get(\"mean\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dritter Durchlauf (PTB-INV Trained oder INV-PTB Trained?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Python39\\lib\\site-packages\\foolbox\\models\\tensorflow.py:13: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-8afdd1068b84>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;31m# Not simultan trainin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[0mvanilla_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"models/vanilla_model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m \u001b[0mres\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mptb_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mptb_acc_to_achieve\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvanilla_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minclude_inv_training\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0miterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[0mptb_acc_arr_not_simultan\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ptb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"accuracy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-fc464ae73042>\u001b[0m in \u001b[0;36mptb_training\u001b[1;34m(ptb_acc_to_achieve, model_to_train, include_inv_training, inclusive_training, use_iterations, iterations)\u001b[0m\n\u001b[0;32m    510\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    511\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m<\u001b[0m\u001b[0miterations\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 512\u001b[1;33m             \u001b[0mres\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_to_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    513\u001b[0m             \u001b[0mptb_acc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ptb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"accuracy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    514\u001b[0m             \u001b[0mclean_acc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"clean\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"accuracy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-fc464ae73042>\u001b[0m in \u001b[0;36mtest_model\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[0my_batch_to_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0madvs_to_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuccess\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_batch_to_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch_to_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilons\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\foolbox\\attacks\\base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    277\u001b[0m         \u001b[0msuccess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreal_epsilons\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 279\u001b[1;33m             \u001b[0mxp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    280\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m             \u001b[1;31m# clip to epsilon because we don't really know what the attack returns;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\foolbox\\attacks\\gradient_descent_base.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_and_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m             \u001b[0mgradients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbounds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mgradient_step_sign\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mstepsize\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mgradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\foolbox\\attacks\\gradient_descent_base.py\u001b[0m in \u001b[0;36mvalue_and_grad\u001b[1;34m(self, loss_fn, x)\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mep\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     ) -> Tuple[ep.Tensor, ep.Tensor]:\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mep\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_and_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     def run(\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\eagerpy\\framework.py\u001b[0m in \u001b[0;36mvalue_and_grad\u001b[1;34m(f, t, *args, **kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m     \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensorType\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensorType\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m ) -> Tuple[TensorType, TensorType]:\n\u001b[1;32m--> 352\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_and_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\eagerpy\\tensor\\tensor.py\u001b[0m in \u001b[0;36mvalue_and_grad\u001b[1;34m(self, f, *args, **kwargs)\u001b[0m\n\u001b[0;32m    541\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensorType\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensorType\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    542\u001b[0m     ) -> Tuple[TensorType, TensorType]:\n\u001b[1;32m--> 543\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value_and_grad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhas_aux\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    544\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    545\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\eagerpy\\tensor\\tensorflow.py\u001b[0m in \u001b[0;36mvalue_and_grad\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    470\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    471\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 472\u001b[1;33m             \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    473\u001b[0m             \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTensorFlowTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mx_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1079\u001b[0m                           for x in nest.flatten(output_gradients)]\n\u001b[0;32m   1080\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1081\u001b[1;33m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[0;32m   1082\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     65\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[0;32m     68\u001b[0m       \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    154\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    157\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\nn_grad.py\u001b[0m in \u001b[0;36m_MaxPoolGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRegisterGradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"MaxPool\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_MaxPoolGrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 674\u001b[1;33m   return gen_nn_ops.max_pool_grad(\n\u001b[0m\u001b[0;32m    675\u001b[0m       \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    676\u001b[0m       \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mmax_pool_grad\u001b[1;34m(orig_input, orig_output, grad, ksize, strides, padding, explicit_paddings, data_format, name)\u001b[0m\n\u001b[0;32m   5800\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5801\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5802\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m   5803\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"MaxPoolGrad\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morig_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morig_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ksize\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5804\u001b[0m         \u001b[0mksize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"strides\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"padding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"explicit_paddings\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# epsilon\n",
    "epsilon=0.3\n",
    "iterations=1500\n",
    "ptb_acc_to_achieve=1\n",
    "\n",
    "# Invariance-Based Adversarial Examples to train, use ONLY THE NEW LABELS\n",
    "inv_advs_to_train=np.load(\"data/invariance_examples/epsilon_0.3/invariance-based_adversarial_examples.npy\")\n",
    "inv_labels_to_train=np.load(\"data/invariance_examples/epsilon_0.3/invariance-based_adversarial_examples_human_labels.npy\")\n",
    "\n",
    "\n",
    "\n",
    "#Handler and writer...\n",
    "handler_simultan_trained = open('data/results/dritter_durchlauf/simultan.csv', 'w',encoding='UTF8',newline='')\n",
    "writer_simultan_trained = csv.writer(handler_simultan_trained)\n",
    "writer_simultan_trained.writerow([\"i\",\"clean_acc\",\"ptb_acc\", \"inv_acc\"])\n",
    "\n",
    "handler_inclusive_trained = open('data/results/dritter_durchlauf/inclusive.csv', 'w',encoding='UTF8',newline='')\n",
    "writer_inclusive_trained = csv.writer(handler_inclusive_trained)\n",
    "writer_inclusive_trained.writerow([\"i\",\"clean_acc\",\"ptb_acc\", \"inv_acc\"])\n",
    "\n",
    "handler_not_simultan_trained = open('data/results/dritter_durchlauf/not_simultan.csv', 'w',encoding='UTF8',newline='')\n",
    "writer_not_simultan_trained = csv.writer(handler_not_simultan_trained)\n",
    "writer_not_simultan_trained.writerow([\"i\",\"clean_acc\",\"ptb_acc\", \"inv_acc\", ])\n",
    "\n",
    "handler_inv_ptb_trained = open('data/results/dritter_durchlauf/inv_ptb.csv', 'w',encoding='UTF8',newline='')\n",
    "writer_inv_ptb_trained = csv.writer(handler_inv_ptb_trained)\n",
    "writer_inv_ptb_trained.writerow([\"i\",\"clean_acc\",\"ptb_acc\", \"inv_acc\", ])\n",
    "\n",
    "handler_ptb_inv_trained = open('data/results/dritter_durchlauf/ptb_inv.csv', 'w',encoding='UTF8',newline='')\n",
    "writer_ptb_inv_trained = csv.writer(handler_ptb_inv_trained)\n",
    "writer_ptb_inv_trained.writerow([\"i\",\"clean_acc\",\"ptb_acc\", \"inv_acc\", ])\n",
    "\n",
    "# inclusive training\n",
    "vanilla_model=load_model(\"models/vanilla_model\")\n",
    "res=ptb_training(ptb_acc_to_achieve, vanilla_model, include_inv_training=False, inclusive_training=True, use_iterations=True, iterations=iterations)\n",
    "\n",
    "ptb_acc_arr_inclusive=res.get(\"ptb\").get(\"accuracy\")\n",
    "inv_acc_arr_inclusive=res.get(\"inv\").get(\"accuracy\")\n",
    "clean_acc_arr_inclusive=res.get(\"clean\").get(\"accuracy\")\n",
    "\n",
    "for i in range(iterations):\n",
    "     data=[i,clean_acc_arr_inclusive[i],ptb_acc_arr_inclusive[i],inv_acc_arr_inclusive[i]]\n",
    "     writer_inclusive_trained.writerow(data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Simultan training\n",
    "vanilla_model=load_model(\"models/vanilla_model\")\n",
    "res=ptb_training(ptb_acc_to_achieve, vanilla_model, include_inv_training=True, use_iterations=True, iterations=iterations)\n",
    "\n",
    "ptb_acc_arr_simultan=res.get(\"ptb\").get(\"accuracy\")\n",
    "inv_acc_arr_simultan=res.get(\"inv\").get(\"accuracy\")\n",
    "clean_acc_arr_simultan=res.get(\"clean\").get(\"accuracy\")\n",
    "\n",
    "\n",
    "for i in range(iterations):\n",
    "    data=[i,clean_acc_arr_simultan[i],ptb_acc_arr_simultan[i],inv_acc_arr_simultan[i]]\n",
    "    writer_simultan_trained.writerow(data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Not simultan trainin\n",
    "vanilla_model=load_model(\"models/vanilla_model\")\n",
    "res=ptb_training(ptb_acc_to_achieve, vanilla_model, include_inv_training=False, use_iterations=True, iterations=iterations)\n",
    "\n",
    "ptb_acc_arr_not_simultan=res.get(\"ptb\").get(\"accuracy\")\n",
    "inv_acc_arr_not_simultan=res.get(\"inv\").get(\"accuracy\")\n",
    "clean_acc_arr_not_simultan=res.get(\"clean\").get(\"accuracy\")\n",
    "\n",
    "for i in range(iterations):\n",
    "    data=[i,clean_acc_arr_not_simultan[i],ptb_acc_arr_not_simultan[i],inv_acc_arr_not_simultan[i]]\n",
    "    writer_not_simultan_trained.writerow(data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"INV-PTB\")\n",
    "\n",
    "\n",
    "vanilla_model=load_model(\"models/vanilla_model\")\n",
    "\n",
    "# first INV-Training...\n",
    "print(\"INV-Training\")\n",
    "vanilla_model.fit(inv_advs_to_train,to_categorical(inv_labels_to_train,num_classes=10),\n",
    "epochs=10,\n",
    "verbose=0)\n",
    "\n",
    "\n",
    "# then PTB-Training\n",
    "res=ptb_training(ptb_acc_to_achieve, vanilla_model,use_iterations=True, iterations=iterations)\n",
    "\n",
    "\n",
    "ptb_acc_arr_inv_ptb=res.get(\"ptb\").get(\"accuracy\")\n",
    "inv_acc_arr_inv_ptb=res.get(\"inv\").get(\"accuracy\")\n",
    "clean_acc_arr_inv_ptb=res.get(\"clean\").get(\"accuracy\")\n",
    "\n",
    "for i in range(iterations):\n",
    "    data=[i,clean_acc_arr_inv_ptb[i],ptb_acc_arr_inv_ptb[i],inv_acc_arr_inv_ptb[i]]\n",
    "    writer_inv_ptb_trained.writerow(data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"PTB-INV\")\n",
    "\n",
    "\n",
    "vanilla_model=load_model(\"models/vanilla_model\")\n",
    "\n",
    "#first PTB-Training\n",
    "res=ptb_training(ptb_acc_to_achieve, vanilla_model,use_iterations=True, iterations=iterations)\n",
    "\n",
    "\n",
    "# then INV-Training\n",
    "print(\"INV-Training\")\n",
    "model.fit(inv_advs_to_train,to_categorical(inv_labels_to_train,num_classes=10),\n",
    "    epochs=10,\n",
    "    verbose=0)\n",
    "\n",
    "\n",
    "ptb_acc_arr_ptb_inv=res.get(\"ptb\").get(\"accuracy\")\n",
    "inv_acc_arr_ptb_inv=res.get(\"inv\").get(\"accuracy\")\n",
    "clean_acc_arr_ptb_inv=res.get(\"clean\").get(\"accuracy\")\n",
    "\n",
    "for i in range(iterations):\n",
    "    data=[i,clean_acc_arr_ptb_inv[i],ptb_acc_arr_ptb_inv[i],inv_acc_arr_ptb_inv[i]]\n",
    "    writer_ptb_inv_trained.writerow(data)\n",
    "\n",
    "\n",
    "\n",
    "handler_not_simultan_trained.close()\n",
    "handler_ptb_inv_trained.close()\n",
    "handler_simultan_trained.close()\n",
    "handler_inv_ptb_trained.close()\n",
    "handler_inclusive_trained.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dritten Durchlauf evaluieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution=5\n",
    "\n",
    "\n",
    "handler_simultan_trained = open('data/results/dritter_durchlauf/simultan.csv', 'r')\n",
    "reader_simultan_trained = csv.DictReader(handler_simultan_trained)\n",
    "\n",
    "handler_not_simultan_trained = open('data/results/dritter_durchlauf/not_simultan.csv', 'r')\n",
    "reader_not_simultan_trained = csv.DictReader(handler_not_simultan_trained)\n",
    "\n",
    "handler_inv_ptb_trained = open('data/results/dritter_durchlauf/inv_ptb.csv', 'r')\n",
    "reader_inv_ptb_trained = csv.DictReader(handler_inv_ptb_trained)\n",
    "\n",
    "handler_ptb_inv_trained = open('data/results/dritter_durchlauf/ptb_inv.csv', 'r')\n",
    "reader_ptb_inv_trained = csv.DictReader(handler_ptb_inv_trained)\n",
    "\n",
    "\n",
    "\n",
    "simultan_clean=[]\n",
    "simultan_ptb=[]\n",
    "simultan_inv=[]\n",
    "\n",
    "simultan_clean_all=[]\n",
    "simultan_ptb_all=[]\n",
    "simultan_inv_all=[]\n",
    "\n",
    "not_simultan_clean=[]\n",
    "not_simultan_ptb=[]\n",
    "not_simultan_inv=[]\n",
    "\n",
    "inv_ptb_clean=[]\n",
    "inv_ptb_ptb=[]\n",
    "inv_ptb_inv=[]\n",
    "\n",
    "ptb_inv_clean=[]\n",
    "ptb_inv_ptb=[]\n",
    "ptb_inv_inv=[]\n",
    "\n",
    "\n",
    "line_count = 0\n",
    "for row in reader_simultan_trained: \n",
    "    simultan_clean_all.append(float(row[\"clean_acc\"]))        \n",
    "    simultan_ptb_all.append(float(row[\"ptb_acc\"]))\n",
    "    simultan_inv_all.append(float(row[\"inv_acc\"]))\n",
    "\n",
    "    if line_count % resolution==0:\n",
    "\n",
    "        simultan_clean.append(float(row[\"clean_acc\"]))        \n",
    "        simultan_ptb.append(float(row[\"ptb_acc\"]))\n",
    "        simultan_inv.append(float(row[\"inv_acc\"]))\n",
    "    line_count+=1\n",
    "\n",
    "line_count = 0\n",
    "for row in reader_not_simultan_trained:\n",
    "    if line_count % resolution==0:\n",
    "        not_simultan_clean.append(float(row[\"clean_acc\"]))\n",
    "        not_simultan_ptb.append(float(row[\"ptb_acc\"]))\n",
    "        not_simultan_inv.append(float(row[\"inv_acc\"]))\n",
    "    line_count+=1\n",
    "\n",
    "line_count = 0\n",
    "for row in reader_inv_ptb_trained:\n",
    "    if line_count % resolution==0:\n",
    "        inv_ptb_clean.append(float(row[\"clean_acc\"]))\n",
    "        inv_ptb_ptb.append(float(row[\"ptb_acc\"]))\n",
    "        inv_ptb_inv.append(float(row[\"inv_acc\"]))\n",
    "    line_count+=1\n",
    "\n",
    "line_count = 0\n",
    "for row in reader_ptb_inv_trained:\n",
    "    if line_count % resolution==0:\n",
    "        ptb_inv_clean.append(float(row[\"clean_acc\"]))\n",
    "        ptb_inv_ptb.append(float(row[\"ptb_acc\"]))\n",
    "        ptb_inv_inv.append(float(row[\"inv_acc\"]))\n",
    "    line_count+=1\n",
    "\n",
    "\n",
    "y=[]\n",
    "i=500\n",
    "j=0\n",
    "while j<1500:\n",
    "    y.append(j)\n",
    "    j+=resolution\n",
    "\n",
    "print(\"Simultan\")  \n",
    "plt.plot( y, simultan_clean, label = \"Clean\")\n",
    "plt.plot( y, simultan_ptb,label = \"PTB\")\n",
    "plt.plot( y, simultan_inv,label = \"INV\")\n",
    "plt.xlabel('Iterationen')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"Not simultan\")\n",
    "plt.plot( y, not_simultan_clean, label = \"Clean\")\n",
    "plt.plot( y, not_simultan_ptb,label = \"PTB\")\n",
    "plt.plot( y, not_simultan_inv,label = \"INV\")\n",
    "plt.xlabel('Iterationen')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# print(\"INV-PTB\")\n",
    "# plt.plot( y, inv_ptb_clean, label = \"Clean\")\n",
    "# plt.plot( y, inv_ptb_ptb,label = \"PTB\")\n",
    "# plt.plot( y, inv_ptb_inv,label = \"INV\")\n",
    "# plt.xlabel('Iterationen')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# print(\"PTB-INV\")\n",
    "# plt.plot( y, ptb_inv_clean, label = \"Clean\")\n",
    "# plt.plot( y, ptb_inv_ptb,label = \"PTB\")\n",
    "# plt.plot( y, ptb_inv_inv,label = \"INV\")\n",
    "# plt.xlabel('Iterationen')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print()\n",
    "best_i=0\n",
    "best_ptb=0.0\n",
    "for i in range(len(simultan_clean_all)):\n",
    "    if simultan_ptb_all[i]>best_ptb:\n",
    "        best_i=i\n",
    "        best_ptb=simultan_ptb_all[i]\n",
    "\n",
    "print(\"simultan beste werte: \")\n",
    "print(\"inv: {}, ptb: {}, i: {}\".format(simultan_inv_all[best_i],simultan_ptb_all[best_i],best_i))\n",
    "\n",
    "print()\n",
    "best_i=0\n",
    "best_ptb=0.0\n",
    "diff=1\n",
    "for i in range(len(simultan_clean_all)):\n",
    "    diff_tmp=simultan_inv_all[i]-simultan_ptb_all[i]\n",
    "    if diff_tmp<diff:\n",
    "        diff=diff_tmp\n",
    "        best_i=i\n",
    "    \n",
    "\n",
    "print(\"Nicht simultan beste werte: \")\n",
    "print(\"inv: {}, ptb: {}, i: {}\".format(simultan_inv_all[best_i],simultan_ptb_all[best_i],best_i))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vierter Durchlauf (Optimalen Wert fÃ¼r Anzahl an Invariance-Based Adversarial Examples finden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checken ob human labels wohl passen...\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# new_labels=np.load(\"data/invariance_examples/epsilon_0.3/invariance-based_adversarial_examples_new_labels.npy\")\n",
    "# human_labels=np.load(\"data/invariance_examples/epsilon_0.3/invariance-based_adversarial_examples_human_labels.npy\")\n",
    "# inv_examples=np.load(\"data/invariance_examples/epsilon_0.3/invariance-based_adversarial_examples.npy\")\n",
    "\n",
    "# fig, axes = plt.subplots(10,10, figsize=(20,20))\n",
    "# for i in range(100):\n",
    "#     ax = axes[i//10,i%10]\n",
    "#     ax.imshow(inv_examples[i], cmap='gray')\n",
    "#     # ax.set_title('Label: {}'.format(inv_labels_to_train[i]))\n",
    "#     ax.set_title('Human: {}, New: {}'.format(human_labels[i], new_labels[i]))\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "for i in range(110):\n",
    "    res1,res2=next_batch(100,x_train,y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
