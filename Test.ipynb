{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports, Variables & Function definitions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "\r\n",
    "\r\n",
    "#Imports\r\n",
    "import tensorflow as tf\r\n",
    "import numpy as np\r\n",
    "from keras.datasets import mnist\r\n",
    "import foolbox as fb\r\n",
    "from keras import callbacks\r\n",
    "from keras.models import Sequential\r\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\r\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\r\n",
    "from tensorflow.keras.utils import to_categorical\r\n",
    "from tensorflow.keras.models import load_model\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "import tensorflow as tf\r\n",
    "from timeit import default_timer as timer\r\n",
    "\r\n",
    "\r\n",
    "#Variables\r\n",
    "epsilon=0.3\r\n",
    "batch_size=1024\r\n",
    "epochs=1000\r\n",
    "pgd_steps=50\r\n",
    "print(\"Tensorflow version: \", tf.__version__)\r\n",
    "print(\"Numpy version: \", np.__version__)\r\n",
    "print(\"Foolbox version: \", fb.__version__)\r\n",
    "tf.config.list_physical_devices('GPU')\r\n",
    "np.random.seed(10)\r\n",
    "assert tf.__version__==\"2.8.0\"\r\n",
    "assert np.__version__==\"1.22.2\"\r\n",
    "assert fb.__version__==\"3.3.1\"\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "#get MNIST data and prepare\r\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n",
    "img_rows = img_cols = 28\r\n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\r\n",
    "x_train = x_train.astype('float32')\r\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\r\n",
    "\r\n",
    "#define variables needed for attacks\r\n",
    "x_attack_to_train = tf.convert_to_tensor(x_train, dtype=tf.float32)\r\n",
    "x_attack_to_train=x_attack_to_train[:,:,:,np.newaxis]\r\n",
    "y_attack_to_train=tf.convert_to_tensor(y_train, dtype=tf.int32)\r\n",
    "\r\n",
    "x_attack_to_test = tf.convert_to_tensor(x_test, dtype=tf.float32)\r\n",
    "x_attack_to_test=x_attack_to_test[:,:,:,np.newaxis]\r\n",
    "y_attack_to_test=tf.convert_to_tensor(y_test, dtype=tf.int32)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "#Functions\r\n",
    "def test_model(model):\r\n",
    "    assert epsilon==0.3\r\n",
    "    inv_advs_to_test=np.load(\"invariance_examples/linf/automated_eps03.npy\")\r\n",
    "    inv_labels_to_test=np.load(\"invariance_examples/linf/automated_eps03_labels.npy\")\r\n",
    "    fmodel=fb.models.tensorflow.TensorFlowModel(model, bounds=(0,1))      \r\n",
    "    attack = fb.attacks.projected_gradient_descent.LinfProjectedGradientDescentAttack(steps=pgd_steps)\r\n",
    "\r\n",
    "    x_batch,y_batch=next_batch(100,x_test,y_test)\r\n",
    "    x_batch_to_test = tf.convert_to_tensor(x_batch, dtype=tf.float32)\r\n",
    "    x_batch_to_test=x_batch_to_test[:,:,:,np.newaxis]\r\n",
    "    y_batch_to_test=tf.convert_to_tensor(y_batch, dtype=tf.int32)\r\n",
    "    _,advs_to_test, success=attack(fmodel,x_batch_to_test, y_batch_to_test, epsilons=epsilon)\r\n",
    "    \r\n",
    "    success_rate=tf.keras.backend.get_value(success).mean(axis=-1).round(2)\r\n",
    "    x=tf.keras.backend.get_value(advs_to_test)\r\n",
    "    ptb_test=x[:, :, :, 0]\r\n",
    "\r\n",
    "    #get accuracies and losses\r\n",
    "    acc =model.evaluate(x_test,to_categorical(y_test), verbose=0)\r\n",
    "    acc_ptb = model.evaluate(ptb_test,to_categorical(y_batch), verbose=0)\r\n",
    "    acc_inv = model.evaluate(inv_advs_to_test,to_categorical(inv_labels_to_test), verbose=0)\r\n",
    "\r\n",
    "\r\n",
    "    # get invariance adversarial examples success rate\r\n",
    "    predictions=model.predict(inv_advs_to_test)\r\n",
    "    disagreeing=0\r\n",
    "    for i in range(len(predictions)):\r\n",
    "        if inv_labels_to_test[i] !=np.argmax(predictions[i]):\r\n",
    "            disagreeing+=1\r\n",
    "              \r\n",
    "    return {\r\n",
    "    \"clean\":{\"loss\": acc[0], \"accuracy\":acc[1]},\r\n",
    "    \"ptb\":{\"loss\": acc_ptb[0], \"accuracy\":acc_ptb[1]},\r\n",
    "    \"inv\":{\"loss\": acc_inv[0], \"accuracy\":acc_inv[1]},\r\n",
    "    \"inv_success_rate\":disagreeing/100}\r\n",
    "\r\n",
    "\r\n",
    "def create_vanilla_model():\r\n",
    "      print(\"creating vanilla model...\")\r\n",
    "      \r\n",
    "      val_images = x_train[:10000]\r\n",
    "      partial_images = x_train[10000:]\r\n",
    "      val_labels = y_train[:10000]\r\n",
    "      partial_labels = y_train[10000:]\r\n",
    "\r\n",
    "      model = Sequential()\r\n",
    "\r\n",
    "      model.add(Conv2D(32, (5, 5), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\r\n",
    "      model.add(MaxPooling2D(pool_size=(2, 2)))\r\n",
    "      model.add(Conv2D(64, (5, 5), activation='relu', kernel_initializer='he_uniform'))\r\n",
    "      model.add(MaxPooling2D(pool_size=(2, 2)))\r\n",
    "      model.add(Flatten())\r\n",
    "      model.add(Dense(1024, activation='relu', kernel_initializer='he_uniform'))\r\n",
    "      model.add(Dense(10, activation='softmax'))\r\n",
    "     \r\n",
    "\r\n",
    "\r\n",
    "      earlystopping = callbacks.EarlyStopping(monitor =\"val_loss\", \r\n",
    "                                        mode =\"min\", patience = 1, \r\n",
    "                                        restore_best_weights = True)\r\n",
    "\r\n",
    "      model.compile(optimizer='RMSprop', loss='categorical_crossentropy', metrics=['accuracy'])\r\n",
    "      print(\"training vanilla model...\")\r\n",
    "      history=model.fit(partial_images,to_categorical(partial_labels),\r\n",
    "                  validation_data =(val_images, to_categorical(val_labels)),\r\n",
    "                  batch_size=batch_size,\r\n",
    "                  epochs=epochs,\r\n",
    "                  shuffle=True,\r\n",
    "                  verbose=2,\r\n",
    "                  callbacks =[earlystopping]\r\n",
    "                  )\r\n",
    "\r\n",
    "      acc = model.evaluate(x_test[0:100],to_categorical(y_test[0:100]))\r\n",
    "      print('BEFORE RETRAIN: Accuracy on clean testing data', acc[1])\r\n",
    "\r\n",
    "      return model\r\n",
    "\r\n",
    "def create_vanilla_model_tramer(filters=64, s1=5, s2=5, s3=3,\r\n",
    "               d1=0, d2=0, fc=256,\r\n",
    "               lr=1e-3, decay=1e-3):\r\n",
    "    model = Sequential()\r\n",
    "    model.add(Conv2D(filters, kernel_size=(s1, s1),\r\n",
    "                     activation='relu',\r\n",
    "                     input_shape=(28, 28, 1)))\r\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\r\n",
    "    model.add(Conv2D(filters*2, (s2, s2), activation='relu'))\r\n",
    "    model.add(BatchNormalization())\r\n",
    "    model.add(Conv2D(filters*2, (s3, s3), activation='relu'))\r\n",
    "    model.add(BatchNormalization())\r\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\r\n",
    "    model.add(Dropout(d1))\r\n",
    "    model.add(Flatten())\r\n",
    "    model.add(Dense(fc, activation='relu'))\r\n",
    "    model.add(Dropout(d2))\r\n",
    "    model.add(Dense(10))\r\n",
    "    \r\n",
    "   \r\n",
    "\r\n",
    "    model.compile(loss='categorical_crossentropy',\r\n",
    "                  optimizer='Adam',\r\n",
    "                  metrics=['accuracy'])\r\n",
    "\r\n",
    "    final = Sequential()\r\n",
    "    final.add(model)\r\n",
    "    final.add(Activation('softmax'))\r\n",
    "    final.compile(loss='categorical_crossentropy',\r\n",
    "                  optimizer='Adam',\r\n",
    "                  metrics=['accuracy'])\r\n",
    "        \r\n",
    "    final.fit(x_train, to_categorical(y_train, 10),\r\n",
    "              batch_size=256,\r\n",
    "              epochs=20,\r\n",
    "              shuffle=True,\r\n",
    "              verbose=2,\r\n",
    "    )\r\n",
    "    return final    \r\n",
    "\r\n",
    "\r\n",
    "# https://stackoverflow.com/questions/40994583/how-to-implement-tensorflows-next-batch-for-own-data\r\n",
    "# Get random batch of data\r\n",
    "def next_batch(num, data, labels):\r\n",
    "    idx = np.arange(0 , len(data))\r\n",
    "    np.random.shuffle(idx)\r\n",
    "    idx = idx[:num]\r\n",
    "    data_shuffle = [data[ i] for i in idx]\r\n",
    "    labels_shuffle = [labels[ i] for i in idx]\r\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create/train vanilla model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "create_vanilla_model().save(\"models/vanilla_model\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Attack Vanilla Model and Retrain with Perturbation-Based Adversarial Examples iteratively\n",
    "Result is ptb_trained_model\n",
    "\n",
    "Takes a few minutes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "#get Model\r\n",
    "model=load_model(\"models/vanilla_model\")\r\n",
    "\r\n",
    "# first attack to evaluate accuracy against it\r\n",
    "fmodel=fb.models.tensorflow.TensorFlowModel(model, bounds=(0,1))      \r\n",
    "attack = fb.attacks.projected_gradient_descent.LinfProjectedGradientDescentAttack(steps=pgd_steps)\r\n",
    "_,advs_to_test, success=attack(fmodel, x_attack_to_test[0:100], y_attack_to_test[0:100], epsilons=epsilon)\r\n",
    "advs_to_test=tf.keras.backend.get_value(advs_to_test)\r\n",
    "\r\n",
    "print(\"Perturbation-based adversarial examples before retraining\")\r\n",
    "_, ax = plt.subplots(10, 10, figsize=(10, 10))\r\n",
    "for i in range(100):\r\n",
    "    temp = np.ones((32, 32, 3), dtype=np.float32)\r\n",
    "    x=advs_to_test[i]\r\n",
    "    x=x[:,:,0]\r\n",
    "    temp[2:-2, 2:-2, 0] = x\r\n",
    "    temp[2:-2, 2:-2, 1] = x\r\n",
    "    temp[2:-2, 2:-2, 2] = x\r\n",
    "    ax[i//10, i%10].imshow(temp)\r\n",
    "    ax[i//10, i%10].axis('off')\r\n",
    "plt.show()\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "#Evaluate clean accuracy on vanilla model\r\n",
    "acc = model.evaluate(x_test[0:100],to_categorical(y_test[0:100]))\r\n",
    "print('BEFORE RETRAIN: Accuracy on clean testing data', acc[1])\r\n",
    "\r\n",
    "#Evaluate accuracy against perturbation-based adversarial examples on vanilla model\r\n",
    "acc = model.evaluate(advs_to_test[0:100],to_categorical(y_test[0:100]))\r\n",
    "print('BEFORE RETRAIN: Accuracy on perturbation-based adversarial examples', acc[1])\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "#Attack and retraining phase\r\n",
    "success_arr=[]\r\n",
    "loss_arr=[]\r\n",
    "accuracy_arr=[]\r\n",
    "x_axis=[]\r\n",
    "success_rate=1\r\n",
    "k=0\r\n",
    "iterations=2500\r\n",
    "attack = fb.attacks.projected_gradient_descent.LinfProjectedGradientDescentAttack(steps=pgd_steps)\r\n",
    "print(\"Attacking and retraining \",iterations,\" times (Can take a couple of minutes):\")\r\n",
    "\r\n",
    "earlystopping = callbacks.EarlyStopping(monitor =\"val_loss\", \r\n",
    "                                        mode =\"min\", patience = 1, \r\n",
    "                                        restore_best_weights = True)\r\n",
    " \r\n",
    "\r\n",
    "for i in range(iterations):\r\n",
    "\r\n",
    "    fmodel=fb.models.tensorflow.TensorFlowModel(model, bounds=(0,1))   \r\n",
    "  \r\n",
    "    x_batch,y_batch=next_batch(100,x_train,y_train)\r\n",
    "    x_batch_to_train = tf.convert_to_tensor(x_batch, dtype=tf.float32)\r\n",
    "    x_batch_to_train=x_batch_to_train[:,:,:,np.newaxis]\r\n",
    "    y_batch_to_train=tf.convert_to_tensor(y_batch, dtype=tf.int32)\r\n",
    "\r\n",
    "    #attack model    \r\n",
    "    _,advs, success=attack(fmodel, x_batch_to_train, y_batch_to_train, epsilons=epsilon) \r\n",
    "    success_rate=tf.keras.backend.get_value(success).mean(axis=-1).round(2)\r\n",
    "    success_arr.append(success_rate) \r\n",
    "   \r\n",
    "    #Retrain model with generated perturbation-based adversarial examples\r\n",
    "    #80% Training 20% Validation\r\n",
    "    x=tf.keras.backend.get_value(advs)\r\n",
    "    x = x[:, :, :, :, 0]\r\n",
    "    x_training=x[0:int(len(x)*0.8)]\r\n",
    "    x_validation=x[int(len(x)*0.8):int(len(x))]\r\n",
    "    y_training=y_batch[0:int(len(x)*0.8)]\r\n",
    "    y_validation=y_batch[int(len(x)*0.8):int(len(x))]\r\n",
    "     \r\n",
    "    history=model.fit(x_training,to_categorical(y_training,num_classes=10),\r\n",
    "        validation_data =(x_validation,to_categorical(y_validation, num_classes=10)),\r\n",
    "        epochs=epochs,\r\n",
    "        verbose=0,\r\n",
    "        callbacks =[earlystopping]\r\n",
    "        )\r\n",
    "    print(i)\r\n",
    "\r\n",
    "model.save(\"models/ptb_trained_model_{}_iterations\".format(iterations))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "1200\n",
      "1201\n",
      "1202\n",
      "1203\n",
      "1204\n",
      "1205\n",
      "1206\n",
      "1207\n",
      "1208\n",
      "1209\n",
      "1210\n",
      "1211\n",
      "1212\n",
      "1213\n",
      "1214\n",
      "1215\n",
      "1216\n",
      "1217\n",
      "1218\n",
      "1219\n",
      "1220\n",
      "1221\n",
      "1222\n",
      "1223\n",
      "1224\n",
      "1225\n",
      "1226\n",
      "1227\n",
      "1228\n",
      "1229\n",
      "1230\n",
      "1231\n",
      "1232\n",
      "1233\n",
      "1234\n",
      "1235\n",
      "1236\n",
      "1237\n",
      "1238\n",
      "1239\n",
      "1240\n",
      "1241\n",
      "1242\n",
      "1243\n",
      "1244\n",
      "1245\n",
      "1246\n",
      "1247\n",
      "1248\n",
      "1249\n",
      "1250\n",
      "1251\n",
      "1252\n",
      "1253\n",
      "1254\n",
      "1255\n",
      "1256\n",
      "1257\n",
      "1258\n",
      "1259\n",
      "1260\n",
      "1261\n",
      "1262\n",
      "1263\n",
      "1264\n",
      "1265\n",
      "1266\n",
      "1267\n",
      "1268\n",
      "1269\n",
      "1270\n",
      "1271\n",
      "1272\n",
      "1273\n",
      "1274\n",
      "1275\n",
      "1276\n",
      "1277\n",
      "1278\n",
      "1279\n",
      "1280\n",
      "1281\n",
      "1282\n",
      "1283\n",
      "1284\n",
      "1285\n",
      "1286\n",
      "1287\n",
      "1288\n",
      "1289\n",
      "1290\n",
      "1291\n",
      "1292\n",
      "1293\n",
      "1294\n",
      "1295\n",
      "1296\n",
      "1297\n",
      "1298\n",
      "1299\n",
      "1300\n",
      "1301\n",
      "1302\n",
      "1303\n",
      "1304\n",
      "1305\n",
      "1306\n",
      "1307\n",
      "1308\n",
      "1309\n",
      "1310\n",
      "1311\n",
      "1312\n",
      "1313\n",
      "1314\n",
      "1315\n",
      "1316\n",
      "1317\n",
      "1318\n",
      "1319\n",
      "1320\n",
      "1321\n",
      "1322\n",
      "1323\n",
      "1324\n",
      "1325\n",
      "1326\n",
      "1327\n",
      "1328\n",
      "1329\n",
      "1330\n",
      "1331\n",
      "1332\n",
      "1333\n",
      "1334\n",
      "1335\n",
      "1336\n",
      "1337\n",
      "1338\n",
      "1339\n",
      "1340\n",
      "1341\n",
      "1342\n",
      "1343\n",
      "1344\n",
      "1345\n",
      "1346\n",
      "1347\n",
      "1348\n",
      "1349\n",
      "1350\n",
      "1351\n",
      "1352\n",
      "1353\n",
      "1354\n",
      "1355\n",
      "1356\n",
      "1357\n",
      "1358\n",
      "1359\n",
      "1360\n",
      "1361\n",
      "1362\n",
      "1363\n",
      "1364\n",
      "1365\n",
      "1366\n",
      "1367\n",
      "1368\n",
      "1369\n",
      "1370\n",
      "1371\n",
      "1372\n",
      "1373\n",
      "1374\n",
      "1375\n",
      "1376\n",
      "1377\n",
      "1378\n",
      "1379\n",
      "1380\n",
      "1381\n",
      "1382\n",
      "1383\n",
      "1384\n",
      "1385\n",
      "1386\n",
      "1387\n",
      "1388\n",
      "1389\n",
      "1390\n",
      "1391\n",
      "1392\n",
      "1393\n",
      "1394\n",
      "1395\n",
      "1396\n",
      "1397\n",
      "1398\n",
      "1399\n",
      "1400\n",
      "1401\n",
      "1402\n",
      "1403\n",
      "1404\n",
      "1405\n",
      "1406\n",
      "1407\n",
      "1408\n",
      "1409\n",
      "1410\n",
      "1411\n",
      "1412\n",
      "1413\n",
      "1414\n",
      "1415\n",
      "1416\n",
      "1417\n",
      "1418\n",
      "1419\n",
      "1420\n",
      "1421\n",
      "1422\n",
      "1423\n",
      "1424\n",
      "1425\n",
      "1426\n",
      "1427\n",
      "1428\n",
      "1429\n",
      "1430\n",
      "1431\n",
      "1432\n",
      "1433\n",
      "1434\n",
      "1435\n",
      "1436\n",
      "1437\n",
      "1438\n",
      "1439\n",
      "1440\n",
      "1441\n",
      "1442\n",
      "1443\n",
      "1444\n",
      "1445\n",
      "1446\n",
      "1447\n",
      "1448\n",
      "1449\n",
      "1450\n",
      "1451\n",
      "1452\n",
      "1453\n",
      "1454\n",
      "1455\n",
      "1456\n",
      "1457\n",
      "1458\n",
      "1459\n",
      "1460\n",
      "1461\n",
      "1462\n",
      "1463\n",
      "1464\n",
      "1465\n",
      "1466\n",
      "1467\n",
      "1468\n",
      "1469\n",
      "1470\n",
      "1471\n",
      "1472\n",
      "1473\n",
      "1474\n",
      "1475\n",
      "1476\n",
      "1477\n",
      "1478\n",
      "1479\n",
      "1480\n",
      "1481\n",
      "1482\n",
      "1483\n",
      "1484\n",
      "1485\n",
      "1486\n",
      "1487\n",
      "1488\n",
      "1489\n",
      "1490\n",
      "1491\n",
      "1492\n",
      "1493\n",
      "1494\n",
      "1495\n",
      "1496\n",
      "1497\n",
      "1498\n",
      "1499\n",
      "1500\n",
      "1501\n",
      "1502\n",
      "1503\n",
      "1504\n",
      "1505\n",
      "1506\n",
      "1507\n",
      "1508\n",
      "1509\n",
      "1510\n",
      "1511\n",
      "1512\n",
      "1513\n",
      "1514\n",
      "1515\n",
      "1516\n",
      "1517\n",
      "1518\n",
      "1519\n",
      "1520\n",
      "1521\n",
      "1522\n",
      "1523\n",
      "1524\n",
      "1525\n",
      "1526\n",
      "1527\n",
      "1528\n",
      "1529\n",
      "1530\n",
      "1531\n",
      "1532\n",
      "1533\n",
      "1534\n",
      "1535\n",
      "1536\n",
      "1537\n",
      "1538\n",
      "1539\n",
      "1540\n",
      "1541\n",
      "1542\n",
      "1543\n",
      "1544\n",
      "1545\n",
      "1546\n",
      "1547\n",
      "1548\n",
      "1549\n",
      "1550\n",
      "1551\n",
      "1552\n",
      "1553\n",
      "1554\n",
      "1555\n",
      "1556\n",
      "1557\n",
      "1558\n",
      "1559\n",
      "1560\n",
      "1561\n",
      "1562\n",
      "1563\n",
      "1564\n",
      "1565\n",
      "1566\n",
      "1567\n",
      "1568\n",
      "1569\n",
      "1570\n",
      "1571\n",
      "1572\n",
      "1573\n",
      "1574\n",
      "1575\n",
      "1576\n",
      "1577\n",
      "1578\n",
      "1579\n",
      "1580\n",
      "1581\n",
      "1582\n",
      "1583\n",
      "1584\n",
      "1585\n",
      "1586\n",
      "1587\n",
      "1588\n",
      "1589\n",
      "1590\n",
      "1591\n",
      "1592\n",
      "1593\n",
      "1594\n",
      "1595\n",
      "1596\n",
      "1597\n",
      "1598\n",
      "1599\n",
      "1600\n",
      "1601\n",
      "1602\n",
      "1603\n",
      "1604\n",
      "1605\n",
      "1606\n",
      "1607\n",
      "1608\n",
      "1609\n",
      "1610\n",
      "1611\n",
      "1612\n",
      "1613\n",
      "1614\n",
      "1615\n",
      "1616\n",
      "1617\n",
      "1618\n",
      "1619\n",
      "1620\n",
      "1621\n",
      "1622\n",
      "1623\n",
      "1624\n",
      "1625\n",
      "1626\n",
      "1627\n",
      "1628\n",
      "1629\n",
      "1630\n",
      "1631\n",
      "1632\n",
      "1633\n",
      "1634\n",
      "1635\n",
      "1636\n",
      "1637\n",
      "1638\n",
      "1639\n",
      "1640\n",
      "1641\n",
      "1642\n",
      "1643\n",
      "1644\n",
      "1645\n",
      "1646\n",
      "1647\n",
      "1648\n",
      "1649\n",
      "1650\n",
      "1651\n",
      "1652\n",
      "1653\n",
      "1654\n",
      "1655\n",
      "1656\n",
      "1657\n",
      "1658\n",
      "1659\n",
      "1660\n",
      "1661\n",
      "1662\n",
      "1663\n",
      "1664\n",
      "1665\n",
      "1666\n",
      "1667\n",
      "1668\n",
      "1669\n",
      "1670\n",
      "1671\n",
      "1672\n",
      "1673\n",
      "1674\n",
      "1675\n",
      "1676\n",
      "1677\n",
      "1678\n",
      "1679\n",
      "1680\n",
      "1681\n",
      "1682\n",
      "1683\n",
      "1684\n",
      "1685\n",
      "1686\n",
      "1687\n",
      "1688\n",
      "1689\n",
      "1690\n",
      "1691\n",
      "1692\n",
      "1693\n",
      "1694\n",
      "1695\n",
      "1696\n",
      "1697\n",
      "1698\n",
      "1699\n",
      "1700\n",
      "1701\n",
      "1702\n",
      "1703\n",
      "1704\n",
      "1705\n",
      "1706\n",
      "1707\n",
      "1708\n",
      "1709\n",
      "1710\n",
      "1711\n",
      "1712\n",
      "1713\n",
      "1714\n",
      "1715\n",
      "1716\n",
      "1717\n",
      "1718\n",
      "1719\n",
      "1720\n",
      "1721\n",
      "1722\n",
      "1723\n",
      "1724\n",
      "1725\n",
      "1726\n",
      "1727\n",
      "1728\n",
      "1729\n",
      "1730\n",
      "1731\n",
      "1732\n",
      "1733\n",
      "1734\n",
      "1735\n",
      "1736\n",
      "1737\n",
      "1738\n",
      "1739\n",
      "1740\n",
      "1741\n",
      "1742\n",
      "1743\n",
      "1744\n",
      "1745\n",
      "1746\n",
      "1747\n",
      "1748\n",
      "1749\n",
      "1750\n",
      "1751\n",
      "1752\n",
      "1753\n",
      "1754\n",
      "1755\n",
      "1756\n",
      "1757\n",
      "1758\n",
      "1759\n",
      "1760\n",
      "1761\n",
      "1762\n",
      "1763\n",
      "1764\n",
      "1765\n",
      "1766\n",
      "1767\n",
      "1768\n",
      "1769\n",
      "1770\n",
      "1771\n",
      "1772\n",
      "1773\n",
      "1774\n",
      "1775\n",
      "1776\n",
      "1777\n",
      "1778\n",
      "1779\n",
      "1780\n",
      "1781\n",
      "1782\n",
      "1783\n",
      "1784\n",
      "1785\n",
      "1786\n",
      "1787\n",
      "1788\n",
      "1789\n",
      "1790\n",
      "1791\n",
      "1792\n",
      "1793\n",
      "1794\n",
      "1795\n",
      "1796\n",
      "1797\n",
      "1798\n",
      "1799\n",
      "1800\n",
      "1801\n",
      "1802\n",
      "1803\n",
      "1804\n",
      "1805\n",
      "1806\n",
      "1807\n",
      "1808\n",
      "1809\n",
      "1810\n",
      "1811\n",
      "1812\n",
      "1813\n",
      "1814\n",
      "1815\n",
      "1816\n",
      "1817\n",
      "1818\n",
      "1819\n",
      "1820\n",
      "1821\n",
      "1822\n",
      "1823\n",
      "1824\n",
      "1825\n",
      "1826\n",
      "1827\n",
      "1828\n",
      "1829\n",
      "1830\n",
      "1831\n",
      "1832\n",
      "1833\n",
      "1834\n",
      "1835\n",
      "1836\n",
      "1837\n",
      "1838\n",
      "1839\n",
      "1840\n",
      "1841\n",
      "1842\n",
      "1843\n",
      "1844\n",
      "1845\n",
      "1846\n",
      "1847\n",
      "1848\n",
      "1849\n",
      "1850\n",
      "1851\n",
      "1852\n",
      "1853\n",
      "1854\n",
      "1855\n",
      "1856\n",
      "1857\n",
      "1858\n",
      "1859\n",
      "1860\n",
      "1861\n",
      "1862\n",
      "1863\n",
      "1864\n",
      "1865\n",
      "1866\n",
      "1867\n",
      "1868\n",
      "1869\n",
      "1870\n",
      "1871\n",
      "1872\n",
      "1873\n",
      "1874\n",
      "1875\n",
      "1876\n",
      "1877\n",
      "1878\n",
      "1879\n",
      "1880\n",
      "1881\n",
      "1882\n",
      "1883\n",
      "1884\n",
      "1885\n",
      "1886\n",
      "1887\n",
      "1888\n",
      "1889\n",
      "1890\n",
      "1891\n",
      "1892\n",
      "1893\n",
      "1894\n",
      "1895\n",
      "1896\n",
      "1897\n",
      "1898\n",
      "1899\n",
      "1900\n",
      "1901\n",
      "1902\n",
      "1903\n",
      "1904\n",
      "1905\n",
      "1906\n",
      "1907\n",
      "1908\n",
      "1909\n",
      "1910\n",
      "1911\n",
      "1912\n",
      "1913\n",
      "1914\n",
      "1915\n",
      "1916\n",
      "1917\n",
      "1918\n",
      "1919\n",
      "1920\n",
      "1921\n",
      "1922\n",
      "1923\n",
      "1924\n",
      "1925\n",
      "1926\n",
      "1927\n",
      "1928\n",
      "1929\n",
      "1930\n",
      "1931\n",
      "1932\n",
      "1933\n",
      "1934\n",
      "1935\n",
      "1936\n",
      "1937\n",
      "1938\n",
      "1939\n",
      "1940\n",
      "1941\n",
      "1942\n",
      "1943\n",
      "1944\n",
      "1945\n",
      "1946\n",
      "1947\n",
      "1948\n",
      "1949\n",
      "1950\n",
      "1951\n",
      "1952\n",
      "1953\n",
      "1954\n",
      "1955\n",
      "1956\n",
      "1957\n",
      "1958\n",
      "1959\n",
      "1960\n",
      "1961\n",
      "1962\n",
      "1963\n",
      "1964\n",
      "1965\n",
      "1966\n",
      "1967\n",
      "1968\n",
      "1969\n",
      "1970\n",
      "1971\n",
      "1972\n",
      "1973\n",
      "1974\n",
      "1975\n",
      "1976\n",
      "1977\n",
      "1978\n",
      "1979\n",
      "1980\n",
      "1981\n",
      "1982\n",
      "1983\n",
      "1984\n",
      "1985\n",
      "1986\n",
      "1987\n",
      "1988\n",
      "1989\n",
      "1990\n",
      "1991\n",
      "1992\n",
      "1993\n",
      "1994\n",
      "1995\n",
      "1996\n",
      "1997\n",
      "1998\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "2023\n",
      "2024\n",
      "2025\n",
      "2026\n",
      "2027\n",
      "2028\n",
      "2029\n",
      "2030\n",
      "2031\n",
      "2032\n",
      "2033\n",
      "2034\n",
      "2035\n",
      "2036\n",
      "2037\n",
      "2038\n",
      "2039\n",
      "2040\n",
      "2041\n",
      "2042\n",
      "2043\n",
      "2044\n",
      "2045\n",
      "2046\n",
      "2047\n",
      "2048\n",
      "2049\n",
      "2050\n",
      "2051\n",
      "2052\n",
      "2053\n",
      "2054\n",
      "2055\n",
      "2056\n",
      "2057\n",
      "2058\n",
      "2059\n",
      "2060\n",
      "2061\n",
      "2062\n",
      "2063\n",
      "2064\n",
      "2065\n",
      "2066\n",
      "2067\n",
      "2068\n",
      "2069\n",
      "2070\n",
      "2071\n",
      "2072\n",
      "2073\n",
      "2074\n",
      "2075\n",
      "2076\n",
      "2077\n",
      "2078\n",
      "2079\n",
      "2080\n",
      "2081\n",
      "2082\n",
      "2083\n",
      "2084\n",
      "2085\n",
      "2086\n",
      "2087\n",
      "2088\n",
      "2089\n",
      "2090\n",
      "2091\n",
      "2092\n",
      "2093\n",
      "2094\n",
      "2095\n",
      "2096\n",
      "2097\n",
      "2098\n",
      "2099\n",
      "2100\n",
      "2101\n",
      "2102\n",
      "2103\n",
      "2104\n",
      "2105\n",
      "2106\n",
      "2107\n",
      "2108\n",
      "2109\n",
      "2110\n",
      "2111\n",
      "2112\n",
      "2113\n",
      "2114\n",
      "2115\n",
      "2116\n",
      "2117\n",
      "2118\n",
      "2119\n",
      "2120\n",
      "2121\n",
      "2122\n",
      "2123\n",
      "2124\n",
      "2125\n",
      "2126\n",
      "2127\n",
      "2128\n",
      "2129\n",
      "2130\n",
      "2131\n",
      "2132\n",
      "2133\n",
      "2134\n",
      "2135\n",
      "2136\n",
      "2137\n",
      "2138\n",
      "2139\n",
      "2140\n",
      "2141\n",
      "2142\n",
      "2143\n",
      "2144\n",
      "2145\n",
      "2146\n",
      "2147\n",
      "2148\n",
      "2149\n",
      "2150\n",
      "2151\n",
      "2152\n",
      "2153\n",
      "2154\n",
      "2155\n",
      "2156\n",
      "2157\n",
      "2158\n",
      "2159\n",
      "2160\n",
      "2161\n",
      "2162\n",
      "2163\n",
      "2164\n",
      "2165\n",
      "2166\n",
      "2167\n",
      "2168\n",
      "2169\n",
      "2170\n",
      "2171\n",
      "2172\n",
      "2173\n",
      "2174\n",
      "2175\n",
      "2176\n",
      "2177\n",
      "2178\n",
      "2179\n",
      "2180\n",
      "2181\n",
      "2182\n",
      "2183\n",
      "2184\n",
      "2185\n",
      "2186\n",
      "2187\n",
      "2188\n",
      "2189\n",
      "2190\n",
      "2191\n",
      "2192\n",
      "2193\n",
      "2194\n",
      "2195\n",
      "2196\n",
      "2197\n",
      "2198\n",
      "2199\n",
      "2200\n",
      "2201\n",
      "2202\n",
      "2203\n",
      "2204\n",
      "2205\n",
      "2206\n",
      "2207\n",
      "2208\n",
      "2209\n",
      "2210\n",
      "2211\n",
      "2212\n",
      "2213\n",
      "2214\n",
      "2215\n",
      "2216\n",
      "2217\n",
      "2218\n",
      "2219\n",
      "2220\n",
      "2221\n",
      "2222\n",
      "2223\n",
      "2224\n",
      "2225\n",
      "2226\n",
      "2227\n",
      "2228\n",
      "2229\n",
      "2230\n",
      "2231\n",
      "2232\n",
      "2233\n",
      "2234\n",
      "2235\n",
      "2236\n",
      "2237\n",
      "2238\n",
      "2239\n",
      "2240\n",
      "2241\n",
      "2242\n",
      "2243\n",
      "2244\n",
      "2245\n",
      "2246\n",
      "2247\n",
      "2248\n",
      "2249\n",
      "2250\n",
      "2251\n",
      "2252\n",
      "2253\n",
      "2254\n",
      "2255\n",
      "2256\n",
      "2257\n",
      "2258\n",
      "2259\n",
      "2260\n",
      "2261\n",
      "2262\n",
      "2263\n",
      "2264\n",
      "2265\n",
      "2266\n",
      "2267\n",
      "2268\n",
      "2269\n",
      "2270\n",
      "2271\n",
      "2272\n",
      "2273\n",
      "2274\n",
      "2275\n",
      "2276\n",
      "2277\n",
      "2278\n",
      "2279\n",
      "2280\n",
      "2281\n",
      "2282\n",
      "2283\n",
      "2284\n",
      "2285\n",
      "2286\n",
      "2287\n",
      "2288\n",
      "2289\n",
      "2290\n",
      "2291\n",
      "2292\n",
      "2293\n",
      "2294\n",
      "2295\n",
      "2296\n",
      "2297\n",
      "2298\n",
      "2299\n",
      "2300\n",
      "2301\n",
      "2302\n",
      "2303\n",
      "2304\n",
      "2305\n",
      "2306\n",
      "2307\n",
      "2308\n",
      "2309\n",
      "2310\n",
      "2311\n",
      "2312\n",
      "2313\n",
      "2314\n",
      "2315\n",
      "2316\n",
      "2317\n",
      "2318\n",
      "2319\n",
      "2320\n",
      "2321\n",
      "2322\n",
      "2323\n",
      "2324\n",
      "2325\n",
      "2326\n",
      "2327\n",
      "2328\n",
      "2329\n",
      "2330\n",
      "2331\n",
      "2332\n",
      "2333\n",
      "2334\n",
      "2335\n",
      "2336\n",
      "2337\n",
      "2338\n",
      "2339\n",
      "2340\n",
      "2341\n",
      "2342\n",
      "2343\n",
      "2344\n",
      "2345\n",
      "2346\n",
      "2347\n",
      "2348\n",
      "2349\n",
      "2350\n",
      "2351\n",
      "2352\n",
      "2353\n",
      "2354\n",
      "2355\n",
      "2356\n",
      "2357\n",
      "2358\n",
      "2359\n",
      "2360\n",
      "2361\n",
      "2362\n",
      "2363\n",
      "2364\n",
      "2365\n",
      "2366\n",
      "2367\n",
      "2368\n",
      "2369\n",
      "2370\n",
      "2371\n",
      "2372\n",
      "2373\n",
      "2374\n",
      "2375\n",
      "2376\n",
      "2377\n",
      "2378\n",
      "2379\n",
      "2380\n",
      "2381\n",
      "2382\n",
      "2383\n",
      "2384\n",
      "2385\n",
      "2386\n",
      "2387\n",
      "2388\n",
      "2389\n",
      "2390\n",
      "2391\n",
      "2392\n",
      "2393\n",
      "2394\n",
      "2395\n",
      "2396\n",
      "2397\n",
      "2398\n",
      "2399\n",
      "2400\n",
      "2401\n",
      "2402\n",
      "2403\n",
      "2404\n",
      "2405\n",
      "2406\n",
      "2407\n",
      "2408\n",
      "2409\n",
      "2410\n",
      "2411\n",
      "2412\n",
      "2413\n",
      "2414\n",
      "2415\n",
      "2416\n",
      "2417\n",
      "2418\n",
      "2419\n",
      "2420\n",
      "2421\n",
      "2422\n",
      "2423\n",
      "2424\n",
      "2425\n",
      "2426\n",
      "2427\n",
      "2428\n",
      "2429\n",
      "2430\n",
      "2431\n",
      "2432\n",
      "2433\n",
      "2434\n",
      "2435\n",
      "2436\n",
      "2437\n",
      "2438\n",
      "2439\n",
      "2440\n",
      "2441\n",
      "2442\n",
      "2443\n",
      "2444\n",
      "2445\n",
      "2446\n",
      "2447\n",
      "2448\n",
      "2449\n",
      "2450\n",
      "2451\n",
      "2452\n",
      "2453\n",
      "2454\n",
      "2455\n",
      "2456\n",
      "2457\n",
      "2458\n",
      "2459\n",
      "2460\n",
      "2461\n",
      "2462\n",
      "2463\n",
      "2464\n",
      "2465\n",
      "2466\n",
      "2467\n",
      "2468\n",
      "2469\n",
      "2470\n",
      "2471\n",
      "2472\n",
      "2473\n",
      "2474\n",
      "2475\n",
      "2476\n",
      "2477\n",
      "2478\n",
      "2479\n",
      "2480\n",
      "2481\n",
      "2482\n",
      "2483\n",
      "2484\n",
      "2485\n",
      "2486\n",
      "2487\n",
      "2488\n",
      "2489\n",
      "2490\n",
      "2491\n",
      "2492\n",
      "2493\n",
      "2494\n",
      "2495\n",
      "2496\n",
      "2497\n",
      "2498\n",
      "2499\n",
      "INFO:tensorflow:Assets written to: models/ptb_trained_model_2500_iterations\\assets\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generate INV-Based ADV-Examples \n",
    "Code is from https://github.com/ftramer/Excessive-Invariance"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save random 200 indices in file...\n",
    "Needed to generate invariance based adversarial examples"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rand_indices = np.random.randint(0,9999,(200))\r\n",
    "np.save(\"data/invariance_examples_generation/random_indices200\", rand_indices)\r\n",
    "\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Find nearest Neighbours (Takes a lot of computation)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from keras.datasets import mnist\r\n",
    "import numpy as np\r\n",
    "from sklearn.neighbors import NearestNeighbors\r\n",
    "from itertools import product\r\n",
    "from scipy.ndimage.interpolation import rotate, shift\r\n",
    "\r\n",
    "#  Load the MNIST data. 100 randomly chosen test points\r\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\r\n",
    "idxs = np.load(\"data/invariance_examples_generation/random_indices200.npy\")\r\n",
    "assert len(idxs) == 200\r\n",
    "test_xs = X_test[idxs]\r\n",
    "test_ys = Y_test[idxs]\r\n",
    "\r\n",
    "# build a nearest neighbors classifier per class\r\n",
    "N = 1\r\n",
    "all_NNs = []\r\n",
    "\r\n",
    "for i in range(10):\r\n",
    "    #Reshape to 1D (28*28=784)\r\n",
    "    X = X_train[Y_train == i].reshape(-1, 784)\r\n",
    "    print(\"X: \", np.shape(X))\r\n",
    "    nn = NearestNeighbors(n_neighbors=N)\r\n",
    "   \r\n",
    "    nn.fit(X)\r\n",
    "    all_NNs.append(nn)\r\n",
    "print(all_NNs)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# Rotation-translation parameters\r\n",
    "limits = [3, 3, 30]\r\n",
    "granularity = [5, 5, 31]\r\n",
    "grid = list(product(*list(np.linspace(-l, l, num=g) for l, g in zip(limits, granularity))))\r\n",
    "\r\n",
    "# tries all rotation-translations of the input and returns the closest neighbor from each class\r\n",
    "def get_best_neighbors(x, y):\r\n",
    "    xs = [shift(rotate(x, r, reshape=False), (tx, ty)).reshape(784) for (tx, ty, r) in grid]\r\n",
    "    xs = np.asarray(xs.copy())\r\n",
    "    \r\n",
    "    nns = []\r\n",
    "    y_nns = []\r\n",
    "    grids_nn = []\r\n",
    "    \r\n",
    "    # find a nearest neighbor in each class\r\n",
    "    for i in range(10):\r\n",
    "        if i != y:\r\n",
    "            X = X_train[Y_train == i]\r\n",
    "            Y = Y_train[Y_train == i]\r\n",
    "            distances, indices = all_NNs[i].kneighbors(xs, n_neighbors=1)\r\n",
    "\r\n",
    "            best = np.argmin(np.reshape(distances, -1))\r\n",
    "            best_idx = np.reshape(indices, -1)[best]\r\n",
    "            nns.append(X[best_idx])\r\n",
    "            y_nns.append(Y[best_idx])\r\n",
    "            \r\n",
    "            # store the inverse rotation+translation to be applied to the target\r\n",
    "            grids_nn.append(-np.asarray(grid[best]))\r\n",
    "    \r\n",
    "    return nns, y_nns, grids_nn\r\n",
    "\r\n",
    "\r\n",
    "all_nns = []\r\n",
    "all_y_nns = []\r\n",
    "all_grids_nns = []\r\n",
    "\r\n",
    "# find nearest neighbors for some test inputs (this takes a little while)\r\n",
    "for i in range(len(idxs)):\r\n",
    "    if i % 10 == 0:\r\n",
    "        print(\"{}/{} done\".format(i, len(idxs)))\r\n",
    "    x = test_xs[i]\r\n",
    "    y = test_ys[i]\r\n",
    "\r\n",
    "    # find the nearest neighbors for each class, with the corresponding rotation and translation\r\n",
    "    nns, y_nns, grids_nns = get_best_neighbors(x, y)\r\n",
    "    nn_advs = [shift(rotate(nn, r, reshape=False), (tx, ty)) for (nn, (tx, ty, r)) in zip(nns, grids_nns)]\r\n",
    "    all_nns.append(nn_advs)\r\n",
    "    all_y_nns.append(y_nns)\r\n",
    "    all_grids_nns.append(np.asarray(grids_nns))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# save everything!\r\n",
    "\r\n",
    "\r\n",
    "np.save(\"data/invariance_examples_generation/X_test_200.npy\", test_xs)\r\n",
    "np.save(\"data/invariance_examples_generation/all_nns.npy\", np.asarray(all_nns))\r\n",
    "np.save(\"data/invariance_examples_generation/all_y_nns.npy\", np.asarray(all_y_nns))\r\n",
    "np.save(\"data/invariance_examples_generation/all_grids_nns.npy\", np.asarray(all_grids_nns))\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def linf_attack(x, nn_adv, eps=epsilon):\r\n",
    "    x_adv = x.copy().astype(np.float32)\r\n",
    "    nn_adv = nn_adv.astype(np.float32)\r\n",
    "    \r\n",
    "    # if possible, change the pixels to the target value\r\n",
    "    idx = np.where((np.abs(nn_adv - x) <= eps*255.) & (x > 0))\r\n",
    "    x_adv[idx] = nn_adv[idx]\r\n",
    "    \r\n",
    "    # otherwise, go as close as possible\r\n",
    "    idx = np.where(np.abs(nn_adv - x) > eps*255.)\r\n",
    "    sign = np.sign(nn_adv - x)\r\n",
    "    x_adv[idx] += sign[idx] * eps * 255.\r\n",
    "    \r\n",
    "    x_adv = np.clip(x_adv, x.astype(np.float32) - eps*255, x.astype(np.float32) + eps*255)\r\n",
    "    x_adv = np.clip(x_adv, 0, 255.)\r\n",
    "    \r\n",
    "    return x_adv"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate invariance based adversarial examples"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "from keras.datasets import mnist\r\n",
    "\r\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\r\n",
    "all_nns=np.load(\"data/invariance_examples_generation/all_nns.npy\")\r\n",
    "all_y_nns=np.load(\"data/invariance_examples_generation/all_y_nns.npy\")\r\n",
    "all_grids_nns=np.load(\"data/invariance_examples_generation/all_grids_nns.npy\")\r\n",
    "test_xs=np.load(\"data/invariance_examples_generation/X_test_200.npy\")\r\n",
    "idxs = np.load(\"data/invariance_examples_generation/random_indices200.npy\")\r\n",
    "assert len(idxs) == 200\r\n",
    "\r\n",
    "test_ys = Y_test[idxs]\r\n",
    "# fig, ax = plt.subplots(max(len(all_nns), 2), len(all_nns[0])+2, \r\n",
    "#                        figsize=(10*2, len(all_nns)*1.5))\r\n",
    "# [a.axis('off') for a in np.asarray(ax).reshape(-1)]\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# manually chosen target classes for each source class\r\n",
    "targets = {\r\n",
    "    0: [4, 6, 8, 9],\r\n",
    "    1: [4, 6, 7, 9],\r\n",
    "    2: [8],\r\n",
    "    3: [8],\r\n",
    "    4: [8, 9],\r\n",
    "    5: [3, 8],\r\n",
    "    6: [0],\r\n",
    "    7: [2, 3],\r\n",
    "    8: [3],\r\n",
    "    9: [3, 4, 5]\r\n",
    "}\r\n",
    "\r\n",
    "best_y_advs = []\r\n",
    "best_targets = []\r\n",
    "best_advs = []\r\n",
    "\r\n",
    "for i in range(len(all_nns)):\r\n",
    "    x = test_xs[i]\r\n",
    "    y = test_ys[i]\r\n",
    " \r\n",
    "    \r\n",
    "    best_x_adv = None\r\n",
    "    best_nn_adv = None\r\n",
    "    amount_removed = []\r\n",
    "    amount_added = []\r\n",
    "    rot = []\r\n",
    "    best_y = None\r\n",
    "    min_removed = np.inf\r\n",
    "    for j in range(len(all_nns[i])):\r\n",
    "        nn_adv = all_nns[i][j]\r\n",
    "        y_nn = all_y_nns[i][j]\r\n",
    "        x_adv = linf_attack(x, nn_adv, eps=0.3)\r\n",
    "        \r\n",
    "        # retain the target that required the least amount of pixels to be \"removed\"\r\n",
    "        curr_rot = np.abs(all_grids_nns[i][j][-1])\r\n",
    "        curr_removed = np.sum(np.abs(np.maximum(x/255. - x_adv/255., 0)))\r\n",
    "        \r\n",
    "        if y_nn in targets[y] and curr_removed < min_removed:\r\n",
    "            min_removed = curr_removed\r\n",
    "            best_y = y_nn\r\n",
    "            best_x_adv = x_adv\r\n",
    "            best_nn_adv = (nn_adv, y_nn)\r\n",
    "                \r\n",
    "    best_targets.append(best_nn_adv)\r\n",
    "    best_advs.append(best_x_adv)\r\n",
    "    best_y_advs.append(best_y)\r\n",
    "    \r\n",
    "   \r\n",
    "        \r\n",
    "print(np.shape(best_advs))\r\n",
    "\r\n",
    "np.save(\"data/invariance-based_adversarial_examples\", best_advs)\r\n",
    "np.save(\"data/invariance-based_adversarial_examples_labels\", best_y_advs)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Retrain Model with Invariance-Based Adversarial Examples"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Load data and model\r\n",
    "model=load_model(\"models/ptb_trained_model_500_iterations\")\r\n",
    "inv_advs_to_train=np.load(\"data/invariance-based_adversarial_examples.npy\")\r\n",
    "inv_labels_to_train=np.load(\"data/invariance-based_adversarial_examples_labels.npy\")\r\n",
    "inv_advs_to_test=np.load(\"invariance_examples/linf/automated_eps03.npy\")\r\n",
    "inv_labels_to_test=np.load(\"invariance_examples/linf/automated_eps03_labels.npy\")\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "#execute ptb-attack\r\n",
    "print(np.shape(inv_advs_to_train))\r\n",
    "fmodel=fb.models.tensorflow.TensorFlowModel(model, bounds=(0,1))      \r\n",
    "attack = fb.attacks.projected_gradient_descent.LinfProjectedGradientDescentAttack(steps=pgd_steps)\r\n",
    "_,advs_to_test, success=attack(fmodel, x_attack_to_test[0:100], y_attack_to_test[0:100], epsilons=epsilon)\r\n",
    "success_rate=tf.keras.backend.get_value(success).mean(axis=-1).round(2)\r\n",
    "print(\"Ptb-Attack success_rate: \",success_rate)\r\n",
    "x=tf.keras.backend.get_value(advs_to_test)\r\n",
    "ptb_test=x[:, :, :, 0]\r\n",
    "\r\n",
    "#evaluate\r\n",
    "acc = model.evaluate(x_test,to_categorical(y_test), verbose=0)\r\n",
    "acc_ptb = model.evaluate(ptb_test,to_categorical(y_test[0:100]), verbose=0)\r\n",
    "acc_inv = model.evaluate(inv_advs_to_test,to_categorical(inv_labels_to_test), verbose=0)\r\n",
    "\r\n",
    "print(\"BEFORE INV-TRAINING:\")\r\n",
    "print(\"clean testing data: \", acc[1])\r\n",
    "print(\"ptb testing data: \", acc_ptb[1])\r\n",
    "print(\"inv testing data: \", acc_inv[1])\r\n",
    "print(\"\\r\\n------------------------\\r\\n\")\r\n",
    "\r\n",
    "\r\n",
    "#retrain with inv-based adv examples\r\n",
    "history=model.fit(inv_advs_to_train[0:50],to_categorical(inv_labels_to_train[0:50],num_classes=10),\r\n",
    "    epochs=10,\r\n",
    "    verbose=0,)\r\n",
    "\r\n",
    "#evaluate\r\n",
    "acc = model.evaluate(x_test,to_categorical(y_test), verbose=0)\r\n",
    "acc_ptb = model.evaluate(ptb_test,to_categorical(y_test[0:100]), verbose=0)\r\n",
    "acc_inv = model.evaluate(inv_advs_to_test,to_categorical(inv_labels_to_test), verbose=0)\r\n",
    "\r\n",
    "print(\"AFTER INV-TRAINING:\")\r\n",
    "print(\"clean testing data: \", acc[1])\r\n",
    "print(\"ptb testing data: \", acc_ptb[1])\r\n",
    "print(\"inv testing data: \", acc_inv[1])\r\n",
    "print(\"\\r\\n------------------------\\r\\n\")\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Getting optimal iterations for PTB Adversarial Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "models=[\r\n",
    "load_model(\"models/vanilla_model\"),\r\n",
    "load_model(\"models/ptb_trained_model_50_iterations\"),\r\n",
    "load_model(\"models/ptb_trained_model_100_iterations\"),\r\n",
    "load_model(\"models/ptb_trained_model_150_iterations\"),\r\n",
    "load_model(\"models/ptb_trained_model_200_iterations\"),\r\n",
    "load_model(\"models/ptb_trained_model_250_iterations\"),\r\n",
    "load_model(\"models/ptb_trained_model_300_iterations\"),\r\n",
    "load_model(\"models/ptb_trained_model_350_iterations\"),\r\n",
    "load_model(\"models/ptb_trained_model_400_iterations\"),\r\n",
    "load_model(\"models/ptb_trained_model_500_iterations\"),\r\n",
    "load_model(\"models/ptb_trained_model_600_iterations\"),\r\n",
    "load_model(\"models/ptb_trained_model_700_iterations\"),\r\n",
    "load_model(\"models/ptb_trained_model_800_iterations\"),\r\n",
    "load_model(\"models/ptb_trained_model_900_iterations\"),\r\n",
    "load_model(\"models/ptb_trained_model_1000_iterations\"),\r\n",
    "load_model(\"models/ptb_trained_model_1500_iterations\"),\r\n",
    "load_model(\"models/ptb_trained_model_2000_iterations\"),\r\n",
    "# load_model(\"models/ptb_trained_model_2500_iterations\"),\r\n",
    "# load_model(\"models/ptb_trained_model_5000_iterations\"),\r\n",
    "]\r\n",
    "y=[0,50,100,150,200,250,300,350,400,500,600,700,800,900,1000,1500,2000]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "\r\n",
    "\r\n",
    "\r\n",
    "acc_clean_arr=[]\r\n",
    "acc_ptb_arr=[]\r\n",
    "acc_inv_arr=[]\r\n",
    "\r\n",
    "i=0\r\n",
    "for model in models:\r\n",
    "    res=test_model(model)\r\n",
    "    acc_clean_arr.append(res.get(\"clean\").get(\"accuracy\"))\r\n",
    "    acc_ptb_arr.append(res.get(\"ptb\").get(\"accuracy\"))\r\n",
    "    acc_inv_arr.append(res.get(\"inv\").get(\"accuracy\"))\r\n",
    "    print(i)\r\n",
    "    i+=1\r\n",
    "plt.plot( y, acc_clean_arr, label = \"clean\")\r\n",
    "plt.plot( y, acc_ptb_arr,label = \"ptb\")\r\n",
    "plt.xlabel('Iterationen')\r\n",
    "plt.ylabel('Accuracy')\r\n",
    "plt.legend()\r\n",
    "plt.show()\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test Models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def test_model(model):\r\n",
    "    assert epsilon==0.3\r\n",
    "    inv_advs_to_test=np.load(\"invariance_examples/linf/automated_eps03.npy\")\r\n",
    "    inv_labels_to_test=np.load(\"invariance_examples/linf/automated_eps03_labels.npy\")\r\n",
    "    fmodel=fb.models.tensorflow.TensorFlowModel(model, bounds=(0,1))      \r\n",
    "    attack = fb.attacks.projected_gradient_descent.LinfProjectedGradientDescentAttack(steps=pgd_steps)\r\n",
    "\r\n",
    "    x_batch,y_batch=next_batch(100,x_train,y_train)\r\n",
    "    x_batch_to_test = tf.convert_to_tensor(x_batch, dtype=tf.float32)\r\n",
    "    x_batch_to_test=x_batch_to_test[:,:,:,np.newaxis]\r\n",
    "    y_batch_to_test=tf.convert_to_tensor(y_batch, dtype=tf.int32)\r\n",
    "    _,advs_to_test, success=attack(fmodel,x_batch_to_test, y_batch_to_test, epsilons=epsilon)\r\n",
    "  \r\n",
    "\r\n",
    "    success_rate=tf.keras.backend.get_value(success).mean(axis=-1).round(2)\r\n",
    "    x=tf.keras.backend.get_value(advs_to_test)\r\n",
    "    ptb_test=x[:, :, :, 0]\r\n",
    "\r\n",
    "    #get accuracies and losses\r\n",
    "    acc =model.evaluate(x_test,to_categorical(y_test), verbose=0)\r\n",
    "    acc_ptb = model.evaluate(ptb_test,to_categorical(y_batch), verbose=0)\r\n",
    "    acc_inv = model.evaluate(inv_advs_to_test,to_categorical(inv_labels_to_test), verbose=0)\r\n",
    "\r\n",
    "\r\n",
    "    # get invariance adversarial examples success rate\r\n",
    "    predictions=model.predict(inv_advs_to_test)\r\n",
    "    disagreeing=0\r\n",
    "    for i in range(len(predictions)):\r\n",
    "        if inv_labels_to_test[i] !=np.argmax(predictions[i]):\r\n",
    "            disagreeing+=1\r\n",
    "              \r\n",
    "    return {\r\n",
    "    \"clean\":{\"loss\": acc[0], \"accuracy\":acc[1]},\r\n",
    "    \"ptb\":{\"loss\": acc_ptb[0], \"accuracy\":acc_ptb[1]},\r\n",
    "    \"inv\":{\"loss\": acc_inv[0], \"accuracy\":acc_inv[1]},\r\n",
    "    \"inv_success_rate\":disagreeing/100}\r\n",
    "\r\n",
    "\r\n",
    "vanilla_model=load_model(\"models/vanilla_model\")\r\n",
    "ptb_trained_model=load_model(\"models/ptb_trained_model_5000_iterations\")\r\n",
    "\r\n",
    "vanilla=test_model(vanilla_model)\r\n",
    "\r\n",
    "ptb=test_model(ptb_trained_model)\r\n",
    "\r\n",
    "\r\n",
    "print(vanilla)\r\n",
    "print(ptb)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}